import{_,r as o,o as r,c as h,b as e,e as l,w as a,d as s,a as t}from"./app-DFklLwn2.js";const c="/online_notesV2/assets/image-20220628201816396-LKZp5jrN.png",m="/online_notesV2/assets/image-20220628201940536-zUxXigis.png",d="/online_notesV2/assets/image-20220628202019376-Dofxwgps.png",p="/online_notesV2/assets/-16564189436151-mo136Xr0.svg",g="/online_notesV2/assets/image-20220628202430436-0OHDsnoD.png",u="/online_notesV2/assets/image-20220628204135615-QuQuNsDy.png",f="/online_notesV2/assets/image-20220628204859364-BwxX_Eg1.png",x="/online_notesV2/assets/image-20220628204233396-CX05W3-5.png",y="/online_notesV2/assets/image-20220628204256965-BxHeUsdo.png",k="/online_notesV2/assets/image-20220628204342147-DdGuZtHP.png",b="/online_notesV2/assets/image-20220628204424950-DLEwgoE1.png",v="/online_notesV2/assets/image-20220628204803472-DLviy_EA.png",z="/online_notesV2/assets/image-20220628195415624-BYKFgFqo.png",V="/online_notesV2/assets/image-20220628195652966-fN1yAuEw.png",w="/online_notesV2/assets/image-20220628195851091-BxA3kNGM.png",E="/online_notesV2/assets/image-20220628195928191-CcE91GJ0.png",B="/online_notesV2/assets/image-20230711093845604-CM0FIU9d.png",A={},D=e("h1",{id:"线性模型",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#线性模型"},[e("span",null,"线性模型")])],-1),N={class:"table-of-contents"},C=t('<h2 id="_0-参考资料" tabindex="-1"><a class="header-anchor" href="#_0-参考资料"><span>0.参考资料：</span></a></h2><ul><li></li><li>ROC可以更聚焦于模型本身，降低测试集带来的干扰</li><li>生成式模型和判别式模型的区别https://www.zhihu.com/question/20446337/answer/1661760071</li></ul><hr><h2 id="_00-补充数学知识" tabindex="-1"><a class="header-anchor" href="#_00-补充数学知识"><span>00 补充数学知识：</span></a></h2><h3 id="_1-什么是方差的无偏估计" tabindex="-1"><a class="header-anchor" href="#_1-什么是方差的无偏估计"><span>1. 什么是方差的无偏估计</span></a></h3><p>参考：</p>',6),L={href:"https://www.zhihu.com/question/20099757/answer/26586088",target:"_blank",rel:"noopener noreferrer"},I=e("li",null,null,-1),q=t('<h4 id="随机变量期望已知时-计算方差" tabindex="-1"><a class="header-anchor" href="#随机变量期望已知时-计算方差"><span>随机变量期望已知时，计算方差</span></a></h4><p>TBD 需要补充中心极限定理</p><img src="'+c+'" alt="image-20220628201816396" style="zoom:67%;"><h4 id="随机变量期望未知时-方差的有偏估计" tabindex="-1"><a class="header-anchor" href="#随机变量期望未知时-方差的有偏估计"><span>随机变量期望未知时，方差的有偏估计</span></a></h4><img src="'+m+'" alt="image-20220628201940536" style="zoom:67%;"><img src="'+d+'" alt="image-20220628202019376" style="zoom:80%;"><h4 id="方差无偏估计" tabindex="-1"><a class="header-anchor" href="#方差无偏估计"><span>方差无偏估计</span></a></h4>',7),F={href:"https://www.zhihu.com/question/20099757/answer/312670291",target:"_blank",rel:"noopener noreferrer"},S=t('<img src="'+p+'" alt="[公式]" style="zoom:67%;"><img src="'+g+'" alt="image-20220628202430436" style="zoom:47%;"><h3 id="_2-先验后验" tabindex="-1"><a class="header-anchor" href="#_2-先验后验"><span>2. 先验后验</span></a></h3><img src="'+u+'" alt="image-20220628204135615" style="zoom:50%;"><h3 id="_3-概率分布" tabindex="-1"><a class="header-anchor" href="#_3-概率分布"><span>3. 概率分布</span></a></h3><img src="'+f+'" alt="image-20220628204859364" style="zoom:67%;"><h3 id="_4-为什么要最大化似然函数" tabindex="-1"><a class="header-anchor" href="#_4-为什么要最大化似然函数"><span>4. 为什么要最大化似然函数</span></a></h3><ul><li>极大似然估计就是构造一个似然函数，这个似然函数就是样本（所观测到的事件）发生的概率，我们需要做的就是使这个样本发生的概率最大，也就是对未知参数求导，使似然函数取极值。因为只有这样时，样本发生的概率最大。</li><li>线性回归的似然函数是高斯函数。均值为线性模型的输出，方差为高斯白噪声的方差。</li><li>逻辑回归的</li></ul><hr><h2 id="_1-线性回归" tabindex="-1"><a class="header-anchor" href="#_1-线性回归"><span>1. 线性回归</span></a></h2><h3 id="_1-1-视角1-直接选择均方误差为损失函数-最小化损失-最小二乘法" tabindex="-1"><a class="header-anchor" href="#_1-1-视角1-直接选择均方误差为损失函数-最小化损失-最小二乘法"><span>1.1 视角1：直接选择均方误差为损失函数，最小化损失(最小二乘法)</span></a></h3><img src="'+x+'" alt="image-20220628204233396" style="zoom:50%;"><img src="'+y+'" alt="image-20220628204256965" style="zoom:50%;"><h3 id="_1-2-视角2-假定模型输出含有高斯白噪声-高斯函数分布为似然函数-极大似然估计" tabindex="-1"><a class="header-anchor" href="#_1-2-视角2-假定模型输出含有高斯白噪声-高斯函数分布为似然函数-极大似然估计"><span>1.2 视角2：假定模型输出含有高斯白噪声，高斯函数分布为似然函数，极大似然估计</span></a></h3><img src="'+k+'" alt="image-20220628204342147" style="zoom:47%;"><img src="'+b+'" alt="image-20220628204424950" style="zoom:47%;"><h3 id="_1-3-正则化其实是对模型参数的最大后验" tabindex="-1"><a class="header-anchor" href="#_1-3-正则化其实是对模型参数的最大后验"><span>1.3 正则化其实是对模型参数的最大后验</span></a></h3><ul><li>L1正则化：参数先验是拉普拉斯分布</li><li>L2正则化：参数先验是高斯分布</li></ul><img src="'+v+'" alt="image-20220628204803472" style="zoom:47%;"><h2 id="_2-逻辑回归-对数几率函数为输出-极大似然法" tabindex="-1"><a class="header-anchor" href="#_2-逻辑回归-对数几率函数为输出-极大似然法"><span>2. 逻辑回归--对数几率函数为输出，极大似然法</span></a></h2><p><strong>核心在于，如何从对数几率函数推导出似然函数，再推导出损失函数与导数</strong></p><ul><li>逻辑回归其实是想用线性模型去完成一个分类任务</li><li>是对数几率函数的线性模型</li><li>因此，选择对数几率函数（sigmoid是形似S的函数，对数几率函数是代表）作为模型的输出，就可以起到二分类的作用</li><li>详细的推导见《机器学习》--周志华</li></ul><h3 id="_2-1-模型的输出" tabindex="-1"><a class="header-anchor" href="#_2-1-模型的输出"><span>2.1 模型的输出</span></a></h3><img src="'+z+'" alt="image-20220628195415624" style="zoom:40%;"><p><strong>把式子中的y视为类别概率的话，我们就可以得出模型的似然函数</strong></p><img src="'+V+'" alt="image-20220628195652966" style="zoom:50%;"><h3 id="_2-2-模型的损失函数及梯度" tabindex="-1"><a class="header-anchor" href="#_2-2-模型的损失函数及梯度"><span>2.2 模型的损失函数及梯度</span></a></h3><p><strong>使用极大似然法，最大化模型的对数似然</strong></p><img src="'+w+'" alt="image-20220628195851091" style="zoom:47%;"><p>而后求出导数就行了</p><img src="'+E+'" alt="image-20220628195928191" style="zoom:67%;"><h2 id="_3-逻辑回归、线性回归与极大似然法" tabindex="-1"><a class="header-anchor" href="#_3-逻辑回归、线性回归与极大似然法"><span>3. 逻辑回归、线性回归与极大似然法</span></a></h2><img src="'+B+'" alt="image-20230711093845604" style="zoom:40%;">',33);function U(X,G){const n=o("router-link"),i=o("ExternalLinkIcon");return r(),h("div",null,[D,e("nav",N,[e("ul",null,[e("li",null,[l(n,{to:"#_0-参考资料"},{default:a(()=>[s("0.参考资料：")]),_:1})]),e("li",null,[l(n,{to:"#_00-补充数学知识"},{default:a(()=>[s("00 补充数学知识：")]),_:1}),e("ul",null,[e("li",null,[l(n,{to:"#_1-什么是方差的无偏估计"},{default:a(()=>[s("1. 什么是方差的无偏估计")]),_:1})]),e("li",null,[l(n,{to:"#_2-先验后验"},{default:a(()=>[s("2. 先验后验")]),_:1})]),e("li",null,[l(n,{to:"#_3-概率分布"},{default:a(()=>[s("3. 概率分布")]),_:1})]),e("li",null,[l(n,{to:"#_4-为什么要最大化似然函数"},{default:a(()=>[s("4. 为什么要最大化似然函数")]),_:1})])])]),e("li",null,[l(n,{to:"#_1-线性回归"},{default:a(()=>[s("1. 线性回归")]),_:1}),e("ul",null,[e("li",null,[l(n,{to:"#_1-1-视角1-直接选择均方误差为损失函数-最小化损失-最小二乘法"},{default:a(()=>[s("1.1 视角1：直接选择均方误差为损失函数，最小化损失(最小二乘法)")]),_:1})]),e("li",null,[l(n,{to:"#_1-2-视角2-假定模型输出含有高斯白噪声-高斯函数分布为似然函数-极大似然估计"},{default:a(()=>[s("1.2 视角2：假定模型输出含有高斯白噪声，高斯函数分布为似然函数，极大似然估计")]),_:1})]),e("li",null,[l(n,{to:"#_1-3-正则化其实是对模型参数的最大后验"},{default:a(()=>[s("1.3 正则化其实是对模型参数的最大后验")]),_:1})])])]),e("li",null,[l(n,{to:"#_2-逻辑回归-对数几率函数为输出-极大似然法"},{default:a(()=>[s("2. 逻辑回归--对数几率函数为输出，极大似然法")]),_:1}),e("ul",null,[e("li",null,[l(n,{to:"#_2-1-模型的输出"},{default:a(()=>[s("2.1 模型的输出")]),_:1})]),e("li",null,[l(n,{to:"#_2-2-模型的损失函数及梯度"},{default:a(()=>[s("2.2 模型的损失函数及梯度")]),_:1})])])]),e("li",null,[l(n,{to:"#_3-逻辑回归、线性回归与极大似然法"},{default:a(()=>[s("3. 逻辑回归、线性回归与极大似然法")]),_:1})])])]),C,e("ul",null,[e("li",null,[e("a",L,[s("为什么样本方差（sample variance）的分母是 n-1"),l(i)])]),I]),q,e("p",null,[e("a",F,[s("参考为什么样本方差（sample variance）的分母是 n-1？ - 马同学的回答 - 知乎"),l(i)])]),S])}const O=_(A,[["render",U],["__file","3_机器学习-1线性模型.html.vue"]]),T=JSON.parse('{"path":"/AI/Basic/3_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html","title":"线性模型","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.参考资料：","slug":"_0-参考资料","link":"#_0-参考资料","children":[]},{"level":2,"title":"00 补充数学知识：","slug":"_00-补充数学知识","link":"#_00-补充数学知识","children":[{"level":3,"title":"1. 什么是方差的无偏估计","slug":"_1-什么是方差的无偏估计","link":"#_1-什么是方差的无偏估计","children":[]},{"level":3,"title":"2. 先验后验","slug":"_2-先验后验","link":"#_2-先验后验","children":[]},{"level":3,"title":"3. 概率分布","slug":"_3-概率分布","link":"#_3-概率分布","children":[]},{"level":3,"title":"4. 为什么要最大化似然函数","slug":"_4-为什么要最大化似然函数","link":"#_4-为什么要最大化似然函数","children":[]}]},{"level":2,"title":"1. 线性回归","slug":"_1-线性回归","link":"#_1-线性回归","children":[{"level":3,"title":"1.1 视角1：直接选择均方误差为损失函数，最小化损失(最小二乘法)","slug":"_1-1-视角1-直接选择均方误差为损失函数-最小化损失-最小二乘法","link":"#_1-1-视角1-直接选择均方误差为损失函数-最小化损失-最小二乘法","children":[]},{"level":3,"title":"1.2 视角2：假定模型输出含有高斯白噪声，高斯函数分布为似然函数，极大似然估计","slug":"_1-2-视角2-假定模型输出含有高斯白噪声-高斯函数分布为似然函数-极大似然估计","link":"#_1-2-视角2-假定模型输出含有高斯白噪声-高斯函数分布为似然函数-极大似然估计","children":[]},{"level":3,"title":"1.3 正则化其实是对模型参数的最大后验","slug":"_1-3-正则化其实是对模型参数的最大后验","link":"#_1-3-正则化其实是对模型参数的最大后验","children":[]}]},{"level":2,"title":"2. 逻辑回归--对数几率函数为输出，极大似然法","slug":"_2-逻辑回归-对数几率函数为输出-极大似然法","link":"#_2-逻辑回归-对数几率函数为输出-极大似然法","children":[{"level":3,"title":"2.1 模型的输出","slug":"_2-1-模型的输出","link":"#_2-1-模型的输出","children":[]},{"level":3,"title":"2.2 模型的损失函数及梯度","slug":"_2-2-模型的损失函数及梯度","link":"#_2-2-模型的损失函数及梯度","children":[]}]},{"level":2,"title":"3. 逻辑回归、线性回归与极大似然法","slug":"_3-逻辑回归、线性回归与极大似然法","link":"#_3-逻辑回归、线性回归与极大似然法","children":[]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/Basic/3_机器学习-1线性模型.md"}');export{O as comp,T as data};
