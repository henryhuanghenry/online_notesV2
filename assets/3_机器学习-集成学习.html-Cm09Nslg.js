import{_ as s,r as o,o as r,c as _,b as l,e,w as n,d as t,a as g}from"./app-DFklLwn2.js";const h="/online_notesV2/assets/IMG_20230709_145637_edit_8984132116779-8885907-DiQVP9hg.jpg",u="/online_notesV2/assets/image-20230709152834813-CaBLldPv.png",c="/online_notesV2/assets/image-20230709151318401-C93CGqnq.png",d="/online_notesV2/assets/image-20230709151350780-w7LrP3Wk.png",p="/online_notesV2/assets/image-20230709152157379-DOffX26u.png",b="/online_notesV2/assets/image-20230709152319138-Il0Pn-aB.png",m="/online_notesV2/assets/image-20230709155223311-Bbb7wt76.png",f="/online_notesV2/assets/image-20230709155642595-DIvxl-jp.png",k={},x=l("h1",{id:"集成学习",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#集成学习"},[l("span",null,"集成学习")])],-1),v={class:"table-of-contents"},B=l("h2",{id:"_0-资料网址",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#_0-资料网址"},[l("span",null,"0.资料网址：")])],-1),z={href:"https://web.njit.edu/~usman/courses/cs675_fall16/BoostedTree.pdf",target:"_blank",rel:"noopener noreferrer"},A={href:"https://blog.csdn.net/v_JULY_v/article/details/81410574",target:"_blank",rel:"noopener noreferrer"},V={href:"https://zhuanlan.zhihu.com/p/82122013",target:"_blank",rel:"noopener noreferrer"},y={href:"https://www.julyedu.com/question/big/kp_id/23/ques_id/2590",target:"_blank",rel:"noopener noreferrer"},E={href:"https://github.com/NLP-LOVE/ML-NLP",target:"_blank",rel:"noopener noreferrer"},w={href:"https://www.cnblogs.com/mantch/p/11164221.html",target:"_blank",rel:"noopener noreferrer"},N=l("p",null,"系列",-1),T={href:"https://zhuanlan.zhihu.com/p/444046237",target:"_blank",rel:"noopener noreferrer"},C={href:"https://zhuanlan.zhihu.com/p/446210663",target:"_blank",rel:"noopener noreferrer"},G={href:"https://zhuanlan.zhihu.com/p/453177733",target:"_blank",rel:"noopener noreferrer"},L={href:"https://zhuanlan.zhihu.com/p/41536315",target:"_blank",rel:"noopener noreferrer"},P=g('<h2 id="_1-boosting和bagging从方差和偏差的角度对比" tabindex="-1"><a class="header-anchor" href="#_1-boosting和bagging从方差和偏差的角度对比"><span>1. boosting和bagging从方差和偏差的角度对比</span></a></h2><p>参考《百面机器学习》</p><p><strong>什么是方差和偏差</strong></p><ul><li>方差和偏差 <ul><li>从整体数据集中采样N次，每次生成样本量为m的数据集，训练出N个模型</li><li>偏差：计算这N个模型输出的均值和真实模型输出的diff--就是偏差</li><li>方差：这N个模型输出的方差</li></ul></li><li>偏差大：有可能是对模型或者学习算法作出了错误的假设</li><li>方差大：模型对于数据集的变化较为敏感，对数据集有过拟合的倾向</li></ul><p><strong>boosting减少偏差，bagging减少方差</strong></p><ul><li>从数学的角度解释：bagging减少方差 <ul><li>bagging是训练的时候对样本进行不同的采样，希望降低不同分类器的相关性。又比如在随机森林的时候，随机选择可选特征的不同子集来进行节点分裂</li></ul></li></ul><img src="'+h+'" alt="IMG_20230709_145637_edit_8984132116779" style="zoom:20%;"><ul><li>而boosting的本质还是在不断的优化损失函数，因此其实是在降低偏差，难以起到降低方差的作用s</li></ul><h2 id="_2-bagging方法" tabindex="-1"><a class="header-anchor" href="#_2-bagging方法"><span>2. bagging方法</span></a></h2><p>目标：需要训练多个尽可能独立的基学习器</p><h3 id="_1-自助法" tabindex="-1"><a class="header-anchor" href="#_1-自助法"><span>1. 自助法</span></a></h3><ul><li>直接基于自助采样法。 <ul><li>每训练一个学习器，都是从全体样本里自助采样一些样本。</li></ul></li></ul><img src="'+u+'" alt="image-20230709152834813" style="zoom:50%;"><h3 id="_2-随机森林" tabindex="-1"><a class="header-anchor" href="#_2-随机森林"><span>2. 随机森林</span></a></h3><ul><li>传统的决策树在决定节点如何split的时候，会使用所有的特征（举例如年龄、性别等） <ul><li>而随机森林在split的时候，会先随机选出一部分特征（即全体特征的子集），而后再进行决策树的计算</li></ul></li></ul><h2 id="_3-常见的boosting方法" tabindex="-1"><a class="header-anchor" href="#_3-常见的boosting方法"><span>3. 常见的boosting方法</span></a></h2><h3 id="_1-adaboost" tabindex="-1"><a class="header-anchor" href="#_1-adaboost"><span>1. adaboost</span></a></h3><p>具体看《机器学习》-周志华</p><ul><li>核心思想是： <ul><li>学习器的权重：为学习器的错误率决定</li><li>样本的权重：由该样本是否被分类正确以及学习器的错误率决定。 <ul><li>若分类正确则减少权重，同时错误率越低减少的越多。</li><li>若被分类错误则增加权重，同时学习器的错误率越高，增加的越多。</li></ul></li></ul></li></ul><img src="'+c+'" alt="image-20230709151318401" style="zoom:40%;"><img src="'+d+'" alt="image-20230709151350780" style="zoom:40%;"><h3 id="_2-gbdt树" tabindex="-1"><a class="header-anchor" href="#_2-gbdt树"><span>2. GBDT树</span></a></h3><ul><li><p>要训练一系列的学习器。</p></li><li><p>给定当前所有的学习器输出，我们希望降低损失函数。</p><ul><li>分类器的输出是自变量，那在当前分类器输出的这个点上，降低损失函数的最快方向就是当前点的负梯度方向。</li><li>因此，下一个分类器的输出，应该就是损失函数在之前所有分类器输出这个点上的负梯度。</li><li><img src="'+p+'" alt="image-20230709152157379" style="zoom:50%;"></li></ul></li><li><p>如果损失函数是均方误差损失函数，</p><ul><li>则负梯度正好是残差</li><li><img src="'+b+'" alt="image-20230709152319138" style="zoom:50%;"></li></ul></li><li><p>优点：</p><ul><li>预测的时候不同的树可以并行计算</li><li>使用决策树作为基学习器，可解释性好</li></ul></li><li><p>缺点：</p><ul><li>训练的时候不同的树是串行的</li><li>稀疏数据可能应对起来较为困难</li></ul></li></ul><h3 id="_3-cart树" tabindex="-1"><a class="header-anchor" href="#_3-cart树"><span>3.CART树</span></a></h3><p>CART树是二叉树，划分准则是 是与否。</p><ul><li><p>对于回归树，特征选择是平方误差最小化</p><ul><li>对于平方误差函数来说，输出值就是变量的均值</li></ul></li><li><p>对于分类树，是基尼系数最小化。</p><ul><li>基尼系数：有放回抽样两次，两次抽样抽到的样本不属于同一类别的概率。</li><li><img src="'+m+'" alt="image-20230709155223311" style="zoom:50%;"></li></ul></li><li><p>每次特征选择时，要找最好的切分变量以及最好的切分点。</p></li></ul><h3 id="_4-xgboost" tabindex="-1"><a class="header-anchor" href="#_4-xgboost"><span>4. xgboost</span></a></h3><ul><li>特点： <ul><li>在损失函数使用二阶泰勒展开</li><li>在损失函数加入了正则化项</li><li>分裂的准则是：最大化损失函数提升的增益（而不像cart树的基尼系数或者均方误差） <ul><li><img src="'+f+'" alt="image-20230709155642595" style="zoom:50%;"></li></ul></li><li>对缺失值有处理策略</li><li>对训练有并行化的优化策略</li></ul></li></ul><h3 id="_5-lightgbm" tabindex="-1"><a class="header-anchor" href="#_5-lightgbm"><span>5. lightGBM</span></a></h3><h3 id="_6-catboost" tabindex="-1"><a class="header-anchor" href="#_6-catboost"><span>6. catboost</span></a></h3><ul><li>https://catboost.ai/en/docs/concepts/fstr</li><li>因为这个特征的分割造成的左右子树的值的方差的加权求和</li></ul>',31);function D(I,M){const i=o("router-link"),a=o("ExternalLinkIcon");return r(),_("div",null,[x,l("nav",v,[l("ul",null,[l("li",null,[e(i,{to:"#_0-资料网址"},{default:n(()=>[t("0.资料网址：")]),_:1})]),l("li",null,[e(i,{to:"#_1-boosting和bagging从方差和偏差的角度对比"},{default:n(()=>[t("1. boosting和bagging从方差和偏差的角度对比")]),_:1})]),l("li",null,[e(i,{to:"#_2-bagging方法"},{default:n(()=>[t("2. bagging方法")]),_:1}),l("ul",null,[l("li",null,[e(i,{to:"#_1-自助法"},{default:n(()=>[t("1. 自助法")]),_:1})]),l("li",null,[e(i,{to:"#_2-随机森林"},{default:n(()=>[t("2. 随机森林")]),_:1})])])]),l("li",null,[e(i,{to:"#_3-常见的boosting方法"},{default:n(()=>[t("3. 常见的boosting方法")]),_:1}),l("ul",null,[l("li",null,[e(i,{to:"#_1-adaboost"},{default:n(()=>[t("1. adaboost")]),_:1})]),l("li",null,[e(i,{to:"#_2-gbdt树"},{default:n(()=>[t("2. GBDT树")]),_:1})]),l("li",null,[e(i,{to:"#_3-cart树"},{default:n(()=>[t("3.CART树")]),_:1})]),l("li",null,[e(i,{to:"#_4-xgboost"},{default:n(()=>[t("4. xgboost")]),_:1})]),l("li",null,[e(i,{to:"#_5-lightgbm"},{default:n(()=>[t("5. lightGBM")]),_:1})]),l("li",null,[e(i,{to:"#_6-catboost"},{default:n(()=>[t("6. catboost")]),_:1})])])])])]),B,l("ul",null,[l("li",null,[l("a",z,[t("tqchen的PPT"),e(a)])]),l("li",null,[l("a",A,[t("csdn博文"),e(a)])]),l("li",null,[l("a",V,[t("知乎博客"),e(a)])]),l("li",null,[l("a",y,[t("好用的刷题网站"),e(a)])]),l("li",null,[l("a",E,[t("github代码+知识点集合"),e(a)])]),l("li",null,[l("a",w,[t("简短一点的博文"),e(a)])])]),N,l("ul",null,[l("li",null,[l("a",T,[e(a)])]),l("li",null,[l("a",C,[e(a)])]),l("li",null,[l("a",G,[e(a)])]),l("li",null,[l("a",L,[t("adaboost"),e(a)])])]),P])}const R=s(k,[["render",D],["__file","3_机器学习-集成学习.html.vue"]]),j=JSON.parse('{"path":"/AI/Basic/3_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html","title":"集成学习","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.资料网址：","slug":"_0-资料网址","link":"#_0-资料网址","children":[]},{"level":2,"title":"1. boosting和bagging从方差和偏差的角度对比","slug":"_1-boosting和bagging从方差和偏差的角度对比","link":"#_1-boosting和bagging从方差和偏差的角度对比","children":[]},{"level":2,"title":"2. bagging方法","slug":"_2-bagging方法","link":"#_2-bagging方法","children":[{"level":3,"title":"1. 自助法","slug":"_1-自助法","link":"#_1-自助法","children":[]},{"level":3,"title":"2. 随机森林","slug":"_2-随机森林","link":"#_2-随机森林","children":[]}]},{"level":2,"title":"3. 常见的boosting方法","slug":"_3-常见的boosting方法","link":"#_3-常见的boosting方法","children":[{"level":3,"title":"1. adaboost","slug":"_1-adaboost","link":"#_1-adaboost","children":[]},{"level":3,"title":"2. GBDT树","slug":"_2-gbdt树","link":"#_2-gbdt树","children":[]},{"level":3,"title":"3.CART树","slug":"_3-cart树","link":"#_3-cart树","children":[]},{"level":3,"title":"4. xgboost","slug":"_4-xgboost","link":"#_4-xgboost","children":[]},{"level":3,"title":"5. lightGBM","slug":"_5-lightgbm","link":"#_5-lightgbm","children":[]},{"level":3,"title":"6. catboost","slug":"_6-catboost","link":"#_6-catboost","children":[]}]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/Basic/3_机器学习-集成学习.md"}');export{R as comp,j as data};
