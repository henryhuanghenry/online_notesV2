import{_ as i,r,o as c,c as u,b as e,e as t,w as o,d as l,a as h}from"./app-DFklLwn2.js";const _="/online_notesV2/assets/image-20240217153614911-CNGSSgOB.png",d="/online_notesV2/assets/image-20240217154303701-PATwjEp6.png",p="/online_notesV2/assets/image-20240217155052515-DQ4v_IHC.png",m="/online_notesV2/assets/image-20240217203846874-L6KJsFTt.png",f="/online_notesV2/assets/image-20240217203607436-lB7E0kVW.png",g="/online_notesV2/assets/image-20240217204525030-B2e6mFWt.png",b={},k=e("h1",{id:"lec1-前置课程-2017-ml-lecture-23-1",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#lec1-前置课程-2017-ml-lecture-23-1"},[e("span",null,"lec1-前置课程-(2017)ML Lecture 23-1")])],-1),v={class:"table-of-contents"},L=e("p",null,"Links:",-1),y={href:"https://www.youtube.com/watch?v=W8XF3ME8G2I",target:"_blank",rel:"noopener noreferrer"},R={href:"https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/RL%20(v4).pdf",target:"_blank",rel:"noopener noreferrer"},x=e("hr",null,null,-1),w=e("h2",{id:"一些前置的大杂烩",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#一些前置的大杂烩"},[e("span",null,"一些前置的大杂烩")])],-1),S=e("p",null,"课程列举的参考资料：",-1),C={href:"http://incompleteideas.net/sutton/book/the-book.html",target:"_blank",rel:"noopener noreferrer"},$={href:"http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html",target:"_blank",rel:"noopener noreferrer"},V={href:"http://videolectures.net/rldm2015_silver_reinforcemen",target:"_blank",rel:"noopener noreferrer"},N={href:"https://youtu.be/aUrX-rP_ss4",target:"_blank",rel:"noopener noreferrer"},D=e("img",{src:_,alt:"image-20240217153614911",style:{zoom:"60%"}},null,-1),E=e("ul",null,[e("li",null,"RL其中一个难点就是reward可能是很稀疏的。")],-1),A=e("img",{src:d,alt:"image-20240217154303701",style:{zoom:"60%"}},null,-1),z=e("ul",null,[e("li",null,"一个episode应该是指一次完整的采样流程。")],-1),B=e("img",{src:p,alt:"image-20240217155052515",style:{zoom:"60%"}},null,-1),M={href:"https://arxiv.org/abs/1312.5602",target:"_blank",rel:"noopener noreferrer"},T={href:"https://proceedings.mlr.press/v48/mniha16.html",target:"_blank",rel:"noopener noreferrer"},I=h('<h2 id="policy-based简述-学习一个actor" tabindex="-1"><a class="header-anchor" href="#policy-based简述-学习一个actor"><span>policy-based简述--学习一个actor</span></a></h2><p>这里的阐述遵循三步走策略：</p><ol><li>对想要学习的东西or function作出一定的假设--此处选了神经网络</li><li>确定评估function好坏的标准</li><li>选出最好的function/根据好坏的标准去学一个较好的function</li></ol><h3 id="actor是什么" tabindex="-1"><a class="header-anchor" href="#actor是什么"><span>actor是什么</span></a></h3><p>可以使用NN作为一个actor</p><ul><li><p>输入是观察到的东西</p></li><li><p>输出是需要采取什么动作</p></li></ul><h3 id="如何衡量actor的好坏" tabindex="-1"><a class="header-anchor" href="#如何衡量actor的好坏"><span>如何衡量actor的好坏</span></a></h3><img src="'+m+'" alt="image-20240217203846874" style="zoom:80%;"><img src="'+f+'" alt="image-20240217203607436" style="zoom:80%;"><ul><li>对概率化的策略来说，一个episode是需要被采样出来的。而采样出来的一个episode被叫做一个轨迹或者一条路径。</li><li>我们可以定义一条轨迹$$\\tau$$的return $$R(\\tau)$$。</li><li>因为一个actor可以采样出多个不同的$$\\tau$$​；因此，多个不同的轨迹的return的期望，就是衡量这个actor好坏的标准。</li></ul><h3 id="如何选出最好的actor" tabindex="-1"><a class="header-anchor" href="#如何选出最好的actor"><span>如何选出最好的actor</span></a></h3><img src="'+g+'" alt="image-20240217204525030" style="zoom:67%;"><ul><li>因为已经有了目标函数，我们只需要调整可学习参数最大化即可。</li></ul><h2 id="value-based简述" tabindex="-1"><a class="header-anchor" href="#value-based简述"><span>value-based简述</span></a></h2>',14);function P(F,J){const a=r("router-link"),s=r("RouteLink"),n=r("ExternalLinkIcon");return c(),u("div",null,[k,e("nav",v,[e("ul",null,[e("li",null,[t(a,{to:"#一些前置的大杂烩"},{default:o(()=>[l("一些前置的大杂烩")]),_:1})]),e("li",null,[t(a,{to:"#policy-based简述-学习一个actor"},{default:o(()=>[l("policy-based简述--学习一个actor")]),_:1}),e("ul",null,[e("li",null,[t(a,{to:"#actor是什么"},{default:o(()=>[l("actor是什么")]),_:1})]),e("li",null,[t(a,{to:"#如何衡量actor的好坏"},{default:o(()=>[l("如何衡量actor的好坏")]),_:1})]),e("li",null,[t(a,{to:"#如何选出最好的actor"},{default:o(()=>[l("如何选出最好的actor")]),_:1})])])]),e("li",null,[t(a,{to:"#value-based简述"},{default:o(()=>[l("value-based简述")]),_:1})])])]),L,e("ul",null,[e("li",null,[t(s,{to:"/CSclass/CSclass_RL_LeeRL/"},{default:o(()=>[l("本笔记README")]),_:1})]),e("li",null,[e("a",y,[l("video"),t(n)])]),e("li",null,[e("a",R,[l("slide"),t(n)])])]),x,w,S,e("ul",null,[e("li",null,[l("Textbook: "),e("a",C,[l("Reinforcement Learning: An Introduction"),t(n)])]),e("li",null,[e("a",$,[l("Lectures of David Silver"),t(n)]),e("ul",null,[e("li",null,[e("a",V,[l("Deep Reinforcement Learning"),t(n)])])])]),e("li",null,[e("a",N,[l("Lectures of John Schulma"),t(n)])])]),D,E,A,z,B,e("ul",null,[e("li",null,[l("课程提到了DQN的方法，以及课程当时最新的A3C方法 "),e("ul",null,[e("li",null,[l("DQN "),e("ul",null,[e("li",null,[e("a",M,[l("Playing Atari with Deep Reinforcement Learning"),t(n)])])])]),e("li",null,[l("A3C "),e("ul",null,[e("li",null,[e("a",T,[l("Asynchronous Methods for Deep Reinforcement Learning"),t(n)])])])])])])]),I])}const W=i(b,[["render",P],["__file","lec1.html.vue"]]),G=JSON.parse('{"path":"/CSclass/CSclass_RL_LeeRL/lec1.html","title":"lec1-前置课程-(2017)ML Lecture 23-1","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"一些前置的大杂烩","slug":"一些前置的大杂烩","link":"#一些前置的大杂烩","children":[]},{"level":2,"title":"policy-based简述--学习一个actor","slug":"policy-based简述-学习一个actor","link":"#policy-based简述-学习一个actor","children":[{"level":3,"title":"actor是什么","slug":"actor是什么","link":"#actor是什么","children":[]},{"level":3,"title":"如何衡量actor的好坏","slug":"如何衡量actor的好坏","link":"#如何衡量actor的好坏","children":[]},{"level":3,"title":"如何选出最好的actor","slug":"如何选出最好的actor","link":"#如何选出最好的actor","children":[]}]},{"level":2,"title":"value-based简述","slug":"value-based简述","link":"#value-based简述","children":[]}],"git":{"updatedTime":1708233876000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"CSclass/CSclass_RL_LeeRL/lec1.md"}');export{W as comp,G as data};
