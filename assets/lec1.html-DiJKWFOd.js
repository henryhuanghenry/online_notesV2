import{_ as i,r as m,o,c,b as s,e as t,w as e,a as n,d as a}from"./app-DFklLwn2.js";const r="/online_notesV2/assets/image-20240129152108137-17065128732031-D9wsxzvw.png",p="/online_notesV2/assets/image-20240129152558622-BxifnpHJ.png",h="/online_notesV2/assets/image-20240129152935049-UX7spRb0.png",u="/online_notesV2/assets/image-20240129165552753-Dv5a8Zl7.png",g="/online_notesV2/assets/image-20240129165612929-hk0R7iml.png",d="/online_notesV2/assets/image-20240129170124498-CBS9LmjN.png",_="/online_notesV2/assets/image-20240129170401046-cja60uMi.png",y="/online_notesV2/assets/image-20240129170506203-Dh36CcbC.png",v="/online_notesV2/assets/image-20240129171608057-CvMT62wa.png",x="/online_notesV2/assets/image-20240129171953394-Bm2bKJvn.png",b="/online_notesV2/assets/image-20240129172541375-DgrxeArO.png",w="/online_notesV2/assets/image-20240129175441973-CT9nXwEs.png",f="/online_notesV2/assets/image-20240129180028086-fr44lXI_.png",z="/online_notesV2/assets/image-20240129180349568-CfPI8DQJ.png",k="/online_notesV2/assets/image-20240129180407606-DBPFvS65.png",M="/online_notesV2/assets/image-20240129180443008-BQnQNNIu.png",S={},V=s("h1",{id:"lec1",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#lec1"},[s("span",null,"lec1")])],-1),L={class:"table-of-contents"},B=n('<p>Links:</p><ul><li>video： <ul><li>【强化学习纲要 第一课 概括与基础 下】 https://www.bilibili.com/video/BV1g7411Z7SJ/?share_source=copy_web&amp;vd_source=09dacd0c6d3992d8c9216969b74b1197</li><li></li></ul></li><li>slides: https://github.com/zhoubolei/introRL/blob/master/lecture1.pdf</li></ul><hr><h1 id="什么是强化学习" tabindex="-1"><a class="header-anchor" href="#什么是强化学习"><span>什么是强化学习</span></a></h1><img src="'+r+'" alt="image-20240129152108137" style="zoom:67%;"><ul><li>agent在跟复杂环境的交互时，最大化获得reward</li></ul><img src="'+p+'" alt="image-20240129152558622" style="zoom:50%;"><p>强化学习和监督学习的区别：</p><ul><li>监督学习假设获取的数据是独立同分布的；而强化学习；获取的数据可能是时序的数据</li><li>监督学习是有标签的，强化学习可能是没有即时反馈的。 <ul><li>强化学习存在delay reward</li></ul></li></ul><h1 id="强化学习的特性" tabindex="-1"><a class="header-anchor" href="#强化学习的特性"><span>强化学习的特性</span></a></h1><img src="'+h+'" alt="image-20240129152935049" style="zoom:67%;"><blockquote><p>因为训练的样本是在训练过程中产生的，因此如何让模型的行为产出的样本能够使得模型能够在训练的过程中效果稳定提升。</p></blockquote><h1 id="序列决策" tabindex="-1"><a class="header-anchor" href="#序列决策"><span>序列决策</span></a></h1><img src="'+u+'" alt="image-20240129165552753" style="zoom:50%;"><h2 id="reward" tabindex="-1"><a class="header-anchor" href="#reward"><span>reward</span></a></h2><img src="'+g+'" alt="image-20240129165612929" style="zoom:50%;">',16),R=s("ul",null,[s("li",null,[a("是一个函数，输出是一个数值量，表示agent在"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"t")]),s("annotation",{encoding:"application/x-tex"},"t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6151em"}}),s("span",{class:"mord mathnormal"},"t")])])]),a("时刻的表现。")]),s("li",null,[a("强化学习是要极大化期望累积reward "),s("ul",null,[s("li",null,"奖励的稀疏程度会影响学习的难度")])])],-1),C=n('<h2 id="序列决策的特点" tabindex="-1"><a class="header-anchor" href="#序列决策的特点"><span>序列决策的特点</span></a></h2><img src="'+d+'" alt="image-20240129170124498" style="zoom:67%;"><ul><li>reward是延后的</li><li>当前的action会造成长期的影响</li><li>如何平衡即时的reward和长期reward <ul><li>如何在当前状态判断长期reward</li></ul></li></ul><h2 id="basic-基本变量" tabindex="-1"><a class="header-anchor" href="#basic-基本变量"><span>Basic--基本变量</span></a></h2><img src="'+_+'" alt="image-20240129170401046" style="zoom:67%;"><img src="'+y+'" alt="image-20240129170506203" style="zoom:67%;">',6),D=s("ul",null,[s("li",null,[s("p",null,[a("state "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"S"),s("mi",null,"t")]),s("mo",null,"="),s("mi",null,"f"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"H"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"S_t = f(H_t)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0813em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])]),a("表征的是，在已有历史"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"H"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"H_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.08125em"}},"H"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0813em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("的情况下，当前的状态是什么样的。")]),s("ul",null,[s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msubsup",null,[s("mi",null,"S"),s("mi",null,"t"),s("mi",null,"e")])]),s("annotation",{encoding:"application/x-tex"},"S_t^e")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9303em","vertical-align":"-0.247em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-2.453em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])]),s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.247em"}},[s("span")])])])])])])])]),a("表示经过历史后，当前的环境状态。同理表示agent状态、")]),s("li",null,"即当前状态由历史通过一个函数")])]),s("li",null,[s("p",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msubsup",null,[s("mi",null,"S"),s("mi",null,"t"),s("mi",null,"e")])]),s("annotation",{encoding:"application/x-tex"},"S_t^e")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9303em","vertical-align":"-0.247em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-2.453em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])]),s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"e")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.247em"}},[s("span")])])])])])])])]),a("和"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msubsup",null,[s("mi",null,"S"),s("mi",null,"t"),s("mi",null,"a")])]),s("annotation",{encoding:"application/x-tex"},"S_t^a")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9303em","vertical-align":"-0.247em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-2.453em","margin-left":"-0.0576em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])]),s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"a")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.247em"}},[s("span")])])])])])])])]),a("这两个之间的关系，代表不同的observability。")]),s("ul",null,[s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",null,"=")]),s("annotation",{encoding:"application/x-tex"},"=")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.3669em"}}),s("span",{class:"mrel"},"=")])])]),a("代表MDP")]),s("li",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",null,"⊋")]),s("annotation",{encoding:"application/x-tex"},"\\supsetneq")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7719em","vertical-align":"-0.136em"}}),s("span",{class:"mrel amsrm"},"⊋")])])]),a("代表POMDP，agent不能完全观测到环境")])])])],-1),E=n('<h2 id="basic-rl三个component" tabindex="-1"><a class="header-anchor" href="#basic-rl三个component"><span>Basic--RL三个component</span></a></h2><img src="'+v+'" alt="image-20240129171608057" style="zoom:67%;"><ul><li>Policy：决策函数，agent根据这个来选取下一步动作</li><li>Value function：价值函数，agent使用这个对当前状态进行评价</li><li>Model：agent对环境的理解？？【TBD】</li></ul><h3 id="policy" tabindex="-1"><a class="header-anchor" href="#policy"><span>policy</span></a></h3><img src="'+x+'" alt="image-20240129171953394" style="zoom:67%;"><ul><li>agent根据状态，决定下一步的动作是什么</li><li>policy也分两种：概率的和确定性的</li></ul><h3 id="value-function" tabindex="-1"><a class="header-anchor" href="#value-function"><span>value function</span></a></h3><img src="'+b+'" alt="image-20240129172541375" style="zoom:80%;"><ul><li>value function是：给定某个policy，在当前状态下？对未来可能获得的奖励的dicount加权的期望 <ul><li>vaule function是从reward的角度，评估policy的好坏</li></ul></li><li>Q-function：在state和action已知的情况下，未来reward的discount加权期望 <ul><li>知道Q函数的话，<mark>当agent进入某个状态时，agent在这个状态的最优action可以通过最大化Q函数得到</mark></li></ul></li></ul><h3 id="model" tabindex="-1"><a class="header-anchor" href="#model"><span>model</span></a></h3><img src="'+w+'" alt="image-20240129175441973" style="zoom:67%;"><ul><li><p>理解为环境模型？</p></li><li><p>预估当前state和action的情况下，下一个state是啥</p></li><li><p>预估当前state和action的情况下，下一个reward是啥</p></li><li><p>？用于在当前state和action计算未来？？【TBD】</p></li></ul><h2 id="basic-不同类型的rlagent" tabindex="-1"><a class="header-anchor" href="#basic-不同类型的rlagent"><span>Basic--不同类型的RLagent</span></a></h2><img src="'+f+'" alt="image-20240129180028086" style="zoom:67%;"><ul><li>value-based：显式学习value function，间接学习了policy</li><li>policy-based：显式学习了policy，不需要学习value function</li></ul><img src="'+z+'" alt="image-20240129180349568" style="zoom:67%;"><img src="'+k+'" alt="image-20240129180407606" style="zoom:67%;"><h2 id="exploration-and-exploitation·" tabindex="-1"><a class="header-anchor" href="#exploration-and-exploitation·"><span>Exploration and Exploitation·</span></a></h2><img src="'+M+'" alt="image-20240129180443008" style="zoom:67%;">',19);function N(H,P){const l=m("router-link");return o(),c("div",null,[V,s("nav",L,[s("ul",null,[s("li",null,[t(l,{to:"#reward"},{default:e(()=>[a("reward")]),_:1})]),s("li",null,[t(l,{to:"#序列决策的特点"},{default:e(()=>[a("序列决策的特点")]),_:1})]),s("li",null,[t(l,{to:"#basic-基本变量"},{default:e(()=>[a("Basic--基本变量")]),_:1})]),s("li",null,[t(l,{to:"#basic-rl三个component"},{default:e(()=>[a("Basic--RL三个component")]),_:1}),s("ul",null,[s("li",null,[t(l,{to:"#policy"},{default:e(()=>[a("policy")]),_:1})]),s("li",null,[t(l,{to:"#value-function"},{default:e(()=>[a("value function")]),_:1})]),s("li",null,[t(l,{to:"#model"},{default:e(()=>[a("model")]),_:1})])])]),s("li",null,[t(l,{to:"#basic-不同类型的rlagent"},{default:e(()=>[a("Basic--不同类型的RLagent")]),_:1})]),s("li",null,[t(l,{to:"#exploration-and-exploitation·"},{default:e(()=>[a("Exploration and Exploitation·")]),_:1})])])]),B,R,C,D,E])}const T=i(S,[["render",N],["__file","lec1.html.vue"]]),J=JSON.parse('{"path":"/CSclass/CSclass_RL_introRL/lec1.html","title":"lec1","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"reward","slug":"reward","link":"#reward","children":[]},{"level":2,"title":"序列决策的特点","slug":"序列决策的特点","link":"#序列决策的特点","children":[]},{"level":2,"title":"Basic--基本变量","slug":"basic-基本变量","link":"#basic-基本变量","children":[]},{"level":2,"title":"Basic--RL三个component","slug":"basic-rl三个component","link":"#basic-rl三个component","children":[{"level":3,"title":"policy","slug":"policy","link":"#policy","children":[]},{"level":3,"title":"value function","slug":"value-function","link":"#value-function","children":[]},{"level":3,"title":"model","slug":"model","link":"#model","children":[]}]},{"level":2,"title":"Basic--不同类型的RLagent","slug":"basic-不同类型的rlagent","link":"#basic-不同类型的rlagent","children":[]},{"level":2,"title":"Exploration and Exploitation·","slug":"exploration-and-exploitation·","link":"#exploration-and-exploitation·","children":[]}],"git":{"updatedTime":1707474668000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"CSclass/CSclass_RL_introRL/lec1.md"}');export{T as comp,J as data};
