import{_ as o,r as i,o as h,c,b as l,e as a,w as n,d as e,a as r}from"./app-DFklLwn2.js";const p="/online_notesV2/assets/1cuOmGT7NevP9oJFJfVpRKA-C_FvSpXf.png",m="/online_notesV2/assets/elmo-2-TNl2SCeN.png",d={},u=l("h1",{id:"基础-词模型",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#基础-词模型"},[l("span",null,"基础 -- 词模型")])],-1),_={class:"table-of-contents"},g=l("h2",{id:"_0-资料网址",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#_0-资料网址"},[l("span",null,"0.资料网址：")])],-1),f={href:"https://easyai.tech/blog/nlp-%E9%A2%86%E5%9F%9F%E9%87%8C%E7%9A%848-%E7%A7%8D%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%96%B9%E5%BC%8F%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9/",target:"_blank",rel:"noopener noreferrer"},b={href:"https://blog.csdn.net/itplus/article/details/37969519",target:"_blank",rel:"noopener noreferrer"},v={href:"http://www.fanyeong.com/2018/02/19/glove-in-detail/#comment-1462",target:"_blank",rel:"noopener noreferrer"},w=r('<h2 id="_1-针对文档的编码" tabindex="-1"><a class="header-anchor" href="#_1-针对文档的编码"><span>1. 针对文档的编码</span></a></h2><h3 id="_1-1-bow-词袋模型" tabindex="-1"><a class="header-anchor" href="#_1-1-bow-词袋模型"><span>1.1 BOW 词袋模型</span></a></h3><p>所有文档的词的集合形成空间，每个文档的向量就是在这个空间中，统计每个词语在该文档中出现的频率</p><ul><li>缺点 <ul><li>无法区分常用词和关键词</li></ul></li></ul><h3 id="_1-2-tf-idf" tabindex="-1"><a class="header-anchor" href="#_1-2-tf-idf"><span>1.2 TF-IDF</span></a></h3><p>有文档，以及文档的集合；可将文档表征为关于词的向量。</p><p>向量的每个值代表了一个词，数值用TF-IDF计算。</p>',7),x=l("ul",null,[l("li",null,[l("p",null,"TF代表该词语在该文档的频率")]),l("li",null,[l("p",null,[e("IDF为逆文档频率，"),l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("mi",null,"l"),l("mi",null,"o"),l("mi",null,"g"),l("mo",{stretchy:"false"},"("),l("mfrac",null,[l("mrow",null,[l("mi",null,"t"),l("mi",null,"o"),l("mi",null,"t"),l("mi",null,"a"),l("mi",null,"l"),l("mi",null,"D"),l("mi",null,"o"),l("mi",null,"c")]),l("mrow",null,[l("mi",null,"a"),l("mi",null,"p"),l("mi",null,"p"),l("mi",null,"e"),l("mi",null,"a"),l("mi",null,"r"),l("mi",null,"D"),l("mi",null,"o"),l("mi",null,"c"),l("mo",null,"+"),l("mn",null,"1")])]),l("mo",{stretchy:"false"},")")]),l("annotation",{encoding:"application/x-tex"},"log(\\frac{totalDoc}{appearDoc+1})")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"1.3612em","vertical-align":"-0.4811em"}}),l("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),l("span",{class:"mord mathnormal"},"o"),l("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),l("span",{class:"mopen"},"("),l("span",{class:"mord"},[l("span",{class:"mopen nulldelimiter"}),l("span",{class:"mfrac"},[l("span",{class:"vlist-t vlist-t2"},[l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.8801em"}},[l("span",{style:{top:"-2.655em"}},[l("span",{class:"pstrut",style:{height:"3em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mtight"},[l("span",{class:"mord mathnormal mtight"},"a"),l("span",{class:"mord mathnormal mtight"},"pp"),l("span",{class:"mord mathnormal mtight"},"e"),l("span",{class:"mord mathnormal mtight"},"a"),l("span",{class:"mord mathnormal mtight"},"rDoc"),l("span",{class:"mbin mtight"},"+"),l("span",{class:"mord mtight"},"1")])])]),l("span",{style:{top:"-3.23em"}},[l("span",{class:"pstrut",style:{height:"3em"}}),l("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),l("span",{style:{top:"-3.394em"}},[l("span",{class:"pstrut",style:{height:"3em"}}),l("span",{class:"sizing reset-size6 size3 mtight"},[l("span",{class:"mord mtight"},[l("span",{class:"mord mathnormal mtight"},"t"),l("span",{class:"mord mathnormal mtight"},"o"),l("span",{class:"mord mathnormal mtight"},"t"),l("span",{class:"mord mathnormal mtight"},"a"),l("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.01968em"}},"l"),l("span",{class:"mord mathnormal mtight"},"Doc")])])])]),l("span",{class:"vlist-s"},"​")]),l("span",{class:"vlist-r"},[l("span",{class:"vlist",style:{height:"0.4811em"}},[l("span")])])])]),l("span",{class:"mclose nulldelimiter"})]),l("span",{class:"mclose"},")")])])]),e("​")])]),l("li",null,[l("p",null,"优点"),l("ul",null,[l("li",null,"实现简单")])]),l("li",null,[l("p",null,"缺点"),l("ul",null,[l("li",null,"不能反映词的位置信息"),l("li",null,"与语料库相关")])])],-1),k=r('<h2 id="_2-针对单词本身的编码" tabindex="-1"><a class="header-anchor" href="#_2-针对单词本身的编码"><span>2.针对单词本身的编码</span></a></h2><h3 id="_2-1-共现矩阵-降维" tabindex="-1"><a class="header-anchor" href="#_2-1-共现矩阵-降维"><span>2.1 共现矩阵+降维</span></a></h3><ul><li><p>使用窗口构成n-gram</p></li><li><p>在同一窗口内的词，两两称为共现</p></li><li><p>用一个矩阵，记录共现的次数</p></li><li><p>然后对该大型矩阵奇异值分解，进行降维，得到词语的矩阵</p></li><li><p>优点</p><ul><li>考虑了出现的顺序</li></ul></li><li><p>缺点</p><ul><li>当词语较多时，构建共现矩阵与降维较为麻烦</li></ul></li></ul><h3 id="_2-2-word2vec" tabindex="-1"><a class="header-anchor" href="#_2-2-word2vec"><span>2.2 word2vec</span></a></h3><p><img src="'+p+'" alt="img"></p><h4 id="skip-gram" tabindex="-1"><a class="header-anchor" href="#skip-gram"><span>skip-gram</span></a></h4><p>对于每个词，目标词的梯度要反向传播K次的。即一个词要训练K次。</p><ul><li>对低频词有好处。</li></ul><h4 id="cbow" tabindex="-1"><a class="header-anchor" href="#cbow"><span>cbow</span></a></h4><p>对于每个词，只训练一次，为这个词的梯度反向传播到K个词。</p><ul><li>因为预测是周围的词加权在一起，所以低频次起的作用可能被高频词覆盖了。</li></ul><h4 id="层次化softmax" tabindex="-1"><a class="header-anchor" href="#层次化softmax"><span>层次化softmax</span></a></h4><p>https://zhuanlan.zhihu.com/p/56139075</p><p>http://www.hankcs.com/nlp/word2vec.html#respond</p><ul><li>根据词频生成一颗哈夫曼树</li><li>而后哈夫曼树向左向右走是由参数来决定的。 <ul><li>每次向左还是向右，根据这一层的概率来决定。</li></ul></li><li>层次softmax在一次前向+反向传播计算过程中，只需要计算从根节点到ground truth label对应的word这条路径上的概率，根据极大似然原理，使这条路径概率最大化，而不用每一个叶子节点都去计算一遍从根节点到叶节点路径的概率。和负采样一样，负采样是挑选了N个单词作为负例，然后根据极大似然估计原理，使得ground truth label对应的word成为正例的概率最大，并使得这N个单词成为负例的概率最大。而层次softmax仅仅是使得ground truth label这条路径的概率最大，就有点像lstm+crf模型中用到的负对数似然损失一样，每一次前向+反向的过程就是使得ground truth对应的标签序列这条路径概率最大</li></ul><h4 id="负采样" tabindex="-1"><a class="header-anchor" href="#负采样"><span>负采样</span></a></h4><p>https://www.cnblogs.com/pinard/p/7249903.html</p><h3 id="_2-3-glove" tabindex="-1"><a class="header-anchor" href="#_2-3-glove"><span>2.3 Glove</span></a></h3><p>基于词向量的点积去拟合共现概率比https://blog.51cto.com/u_15054042/2564022</p><h3 id="_2-4-elmo" tabindex="-1"><a class="header-anchor" href="#_2-4-elmo"><span>2.4 ELMo</span></a></h3><p><img src="'+m+'" alt="img"></p><p>https://zhuanlan.zhihu.com/p/63115885</p><p>利用两个LSTM，一个编码上文信息，一个编码下文信息；每个目标单词利用上下文信息去预测。</p><p>词向量为：多层+上+下文信息的两个hidden state，加上单词本身的Hidden state</p><ul><li>缺点，没有transformer的信息抽取能力强。</li><li>双向拼接融合特征的方法可能没有那么的好</li></ul><h3 id="_2-5-gpt" tabindex="-1"><a class="header-anchor" href="#_2-5-gpt"><span>2.5 GPT</span></a></h3><h3 id="_2-6-bert" tabindex="-1"><a class="header-anchor" href="#_2-6-bert"><span>2.6 Bert</span></a></h3><ul><li>transformer的encoder</li><li>WordPiece embeddings</li><li>Masked LM (15%) <ul><li>fine-tune的时候mask不会出现：80%mask，10换其他，10不变</li></ul></li><li>Next Sentence Prediction (NSP)</li></ul><h3 id="_2-7-roberta" tabindex="-1"><a class="header-anchor" href="#_2-7-roberta"><span>2.7 roberta</span></a></h3><ul><li>Byte-Pair Encoding (BPE)</li><li>Dynamic Masking</li><li>Training with large batches</li><li>更长的句子using individual sentences hurts performance on downstream tasks</li><li>removing the NSP loss matches or slightly improves downstream task performance</li></ul><h3 id="_2-8-albert" tabindex="-1"><a class="header-anchor" href="#_2-8-albert"><span>2.8 albert</span></a></h3><p>减少参数数量，加速训练</p><ul><li>Factorized embedding parameterization <ul><li>Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space</li></ul></li><li>Cross-layer parameter sharing</li><li>Inter-sentence coherence loss 预测是否为下一个句子</li></ul>',33);function E(y,B){const t=i("router-link"),s=i("ExternalLinkIcon");return h(),c("div",null,[u,l("nav",_,[l("ul",null,[l("li",null,[a(t,{to:"#_0-资料网址"},{default:n(()=>[e("0.资料网址：")]),_:1})]),l("li",null,[a(t,{to:"#_1-针对文档的编码"},{default:n(()=>[e("1. 针对文档的编码")]),_:1}),l("ul",null,[l("li",null,[a(t,{to:"#_1-1-bow-词袋模型"},{default:n(()=>[e("1.1 BOW 词袋模型")]),_:1})]),l("li",null,[a(t,{to:"#_1-2-tf-idf"},{default:n(()=>[e("1.2 TF-IDF")]),_:1})])])]),l("li",null,[a(t,{to:"#_2-针对单词本身的编码"},{default:n(()=>[e("2.针对单词本身的编码")]),_:1}),l("ul",null,[l("li",null,[a(t,{to:"#_2-1-共现矩阵-降维"},{default:n(()=>[e("2.1 共现矩阵+降维")]),_:1})]),l("li",null,[a(t,{to:"#_2-2-word2vec"},{default:n(()=>[e("2.2 word2vec")]),_:1})]),l("li",null,[a(t,{to:"#_2-3-glove"},{default:n(()=>[e("2.3 Glove")]),_:1})]),l("li",null,[a(t,{to:"#_2-4-elmo"},{default:n(()=>[e("2.4 ELMo")]),_:1})]),l("li",null,[a(t,{to:"#_2-5-gpt"},{default:n(()=>[e("2.5 GPT")]),_:1})]),l("li",null,[a(t,{to:"#_2-6-bert"},{default:n(()=>[e("2.6 Bert")]),_:1})]),l("li",null,[a(t,{to:"#_2-7-roberta"},{default:n(()=>[e("2.7 roberta")]),_:1})]),l("li",null,[a(t,{to:"#_2-8-albert"},{default:n(()=>[e("2.8 albert")]),_:1})])])])])]),g,l("ul",null,[l("li",null,[l("a",f,[e("NLP 领域里的8 种文本表示方式及优缺点"),a(s)])]),l("li",null,[l("a",b,[e("word2vec的数学原理"),a(s)])]),l("li",null,[l("a",v,[e("glove详解"),a(s)])])]),w,x,k])}const A=o(d,[["render",E],["__file","基础 -- 词模型.html.vue"]]),N=JSON.parse('{"path":"/AI/NLP/%E5%9F%BA%E7%A1%80%20--%20%E8%AF%8D%E6%A8%A1%E5%9E%8B.html","title":"基础 -- 词模型","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.资料网址：","slug":"_0-资料网址","link":"#_0-资料网址","children":[]},{"level":2,"title":"1. 针对文档的编码","slug":"_1-针对文档的编码","link":"#_1-针对文档的编码","children":[{"level":3,"title":"1.1 BOW 词袋模型","slug":"_1-1-bow-词袋模型","link":"#_1-1-bow-词袋模型","children":[]},{"level":3,"title":"1.2 TF-IDF","slug":"_1-2-tf-idf","link":"#_1-2-tf-idf","children":[]}]},{"level":2,"title":"2.针对单词本身的编码","slug":"_2-针对单词本身的编码","link":"#_2-针对单词本身的编码","children":[{"level":3,"title":"2.1 共现矩阵+降维","slug":"_2-1-共现矩阵-降维","link":"#_2-1-共现矩阵-降维","children":[]},{"level":3,"title":"2.2 word2vec","slug":"_2-2-word2vec","link":"#_2-2-word2vec","children":[]},{"level":3,"title":"2.3 Glove","slug":"_2-3-glove","link":"#_2-3-glove","children":[]},{"level":3,"title":"2.4 ELMo","slug":"_2-4-elmo","link":"#_2-4-elmo","children":[]},{"level":3,"title":"2.5 GPT","slug":"_2-5-gpt","link":"#_2-5-gpt","children":[]},{"level":3,"title":"2.6 Bert","slug":"_2-6-bert","link":"#_2-6-bert","children":[]},{"level":3,"title":"2.7 roberta","slug":"_2-7-roberta","link":"#_2-7-roberta","children":[]},{"level":3,"title":"2.8 albert","slug":"_2-8-albert","link":"#_2-8-albert","children":[]}]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/NLP/基础 -- 词模型.md"}');export{A as comp,N as data};
