import{_ as o,r as s,o as r,c as _,b as e,e as n,w as t,d as l,a as c}from"./app-DFklLwn2.js";const d="/online_notesV2/assets/image-20230111223858100-CHfYm7jR.png",h="/online_notesV2/assets/image-20220703170022145-B4hxavw2.png",p="/online_notesV2/assets/image-20220703170222560-Br95-2s1.png",m="/online_notesV2/assets/image-20220703171135294-CZmmh1nt.png",u="/online_notesV2/assets/image-20220703170245345-CnmIvQk8.png",f="/online_notesV2/assets/image-20220703170255013-BLfxz0ob.png",g="/online_notesV2/assets/image-20220703170307202-C-xaqBPc.png",k="/online_notesV2/assets/v2-9c21acd3a39c3bf97d964134ea79710a_r-B_F-srrf.jpg",b="/online_notesV2/assets/image-20230111222145079-D87RSXiB.png",x="/online_notesV2/assets/image-20230111223656332-DWLb0MuU.png",v={},w=e("h1",{id:"基础-transformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#基础-transformer"},[e("span",null,"基础 -- Transformer")])],-1),V={class:"table-of-contents"},y=e("h2",{id:"_0-资料网址",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_0-资料网址"},[e("span",null,"0.资料网址：")])],-1),B={href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"},N={href:"https://classic.d2l.ai/chapter_attention-mechanism/transformer.html",target:"_blank",rel:"noopener noreferrer"},I=c('<h2 id="_0-positional-encodering" tabindex="-1"><a class="header-anchor" href="#_0-positional-encodering"><span>0. positional encodering</span></a></h2><ul><li>参考网址https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</li><li>对于当前posititonal encodering的相对位置的证明https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/</li></ul><h3 id="_0-1-为什么需要positional-encodering-普通的为何不行" tabindex="-1"><a class="header-anchor" href="#_0-1-为什么需要positional-encodering-普通的为何不行"><span>0.1 为什么需要positional encodering，普通的为何不行</span></a></h3><ul><li><p>模型需要对位置有个概念，需要知道单词的先后顺序</p></li><li><p>但是因为attention的存在，所有的单词是同时间输入到模型的</p></li><li><p>为何不能简单的用[0,1]或者[1,2, , length_word]对每个单词进行赋值</p><ul><li>简单的用[0,1]的话，不同句子长度不一样，而每个不同长度的句子，数值间隔步长就不一样 <ul><li>比如长度为5的句子单词间隔0.2，长度为10的句子单词间隔0.1</li><li>这样就导致了，单词的数值间隔的不一致，难以表征顺序，影响学习</li></ul></li><li>如果使用[1,2, , length_word]的话 <ul><li>模型的输入的句子的长度不一致，如果位置编码与长度相关的话，如果模型没见过一些长度的句子，也容易影响学习</li><li>比如常见的都是12长的，如果输入100长的，后面的位置编码模型都没见过</li></ul></li></ul></li></ul><h3 id="_0-2-位置编码需要的特性" tabindex="-1"><a class="header-anchor" href="#_0-2-位置编码需要的特性"><span>0.2 位置编码需要的特性</span></a></h3><blockquote><p>Ideally, the following criteria should be satisfied:</p><ul><li>It should output a unique encoding for each time-step (word’s position in a sentence)</li><li>Distance between any two time-steps should be consistent across sentences with different lengths.</li><li>Our model should generalize to longer sentences without any efforts. Its values should be bounded.</li><li>It must be deterministic.</li></ul></blockquote><ul><li>编码是确定性的，不取决于输入长度</li><li>每一个位置编码唯一</li><li>每两个位置的相对间隔是一致的，如1和2 以及 5和6 之间需要有一致的间隔，比如0.1</li><li>可以使用更长的输入序列</li></ul><h3 id="_0-3-当前的位置编码" tabindex="-1"><a class="header-anchor" href="#_0-3-当前的位置编码"><span>0.3 当前的位置编码</span></a></h3><p><img src="'+d+'" alt="image-20230111223858100"></p><p><img src="'+h+'" alt="image-20220703170022145"></p><h2 id="_1-self-attention" tabindex="-1"><a class="header-anchor" href="#_1-self-attention"><span>1. self-attention</span></a></h2><img src="'+p+'" alt="image-20220703170222560" style="zoom:150%;"><h3 id="_1-1-归一化" tabindex="-1"><a class="header-anchor" href="#_1-1-归一化"><span>1.1 归一化</span></a></h3><p>为何要除以根号dk，是因为，相乘求和之后，方差为变大变为原来的dk倍数。</p><ul><li>因此，除以根号dk其实是一个z的归一化，使其方差继续变为1</li></ul><h3 id="_1-2-如何mask" tabindex="-1"><a class="header-anchor" href="#_1-2-如何mask"><span>1.2 如何mask</span></a></h3><p><img src="'+m+'" alt="image-20220703171135294"></p><h2 id="_2-多头注意力" tabindex="-1"><a class="header-anchor" href="#_2-多头注意力"><span>2. 多头注意力</span></a></h2><p><img src="'+u+'" alt="image-20220703170245345"></p><h2 id="_3-point-wise-ffn" tabindex="-1"><a class="header-anchor" href="#_3-point-wise-ffn"><span>3. Point-wise FFN</span></a></h2><img src="'+f+'" alt="image-20220703170255013" style="zoom:150%;"><h2 id="_4-模型参数计算" tabindex="-1"><a class="header-anchor" href="#_4-模型参数计算"><span>4. 模型参数计算</span></a></h2><img src="'+g+'" alt="image-20220703170307202" style="zoom:80%;"><h2 id="_5-transformer结构" tabindex="-1"><a class="header-anchor" href="#_5-transformer结构"><span>5 transformer结构</span></a></h2><p><img src="'+k+'" alt="img"></p><img src="'+b+'" alt="image-20230111222145079" style="zoom:50%;"><p><img src="'+x+'" alt="image-20230111223656332"></p><p>时间复杂度指的是attention的时间复杂度，即求出所有attention值的时间复杂度</p><h2 id="_6-和rnn对比" tabindex="-1"><a class="header-anchor" href="#_6-和rnn对比"><span>6. 和rnn对比</span></a></h2><ul><li>attention的计算可以并行化实现</li><li>可以设置多个head，学习不同角度的知识</li><li>缺点： <ul><li>位置编码的信息不够直观</li></ul></li></ul>',30);function z(T,F){const i=s("router-link"),a=s("ExternalLinkIcon");return r(),_("div",null,[w,e("nav",V,[e("ul",null,[e("li",null,[n(i,{to:"#_0-资料网址"},{default:t(()=>[l("0.资料网址：")]),_:1})]),e("li",null,[n(i,{to:"#_0-positional-encodering"},{default:t(()=>[l("0. positional encodering")]),_:1}),e("ul",null,[e("li",null,[n(i,{to:"#_0-1-为什么需要positional-encodering-普通的为何不行"},{default:t(()=>[l("0.1 为什么需要positional encodering，普通的为何不行")]),_:1})]),e("li",null,[n(i,{to:"#_0-2-位置编码需要的特性"},{default:t(()=>[l("0.2 位置编码需要的特性")]),_:1})]),e("li",null,[n(i,{to:"#_0-3-当前的位置编码"},{default:t(()=>[l("0.3 当前的位置编码")]),_:1})])])]),e("li",null,[n(i,{to:"#_1-self-attention"},{default:t(()=>[l("1. self-attention")]),_:1}),e("ul",null,[e("li",null,[n(i,{to:"#_1-1-归一化"},{default:t(()=>[l("1.1 归一化")]),_:1})]),e("li",null,[n(i,{to:"#_1-2-如何mask"},{default:t(()=>[l("1.2 如何mask")]),_:1})])])]),e("li",null,[n(i,{to:"#_2-多头注意力"},{default:t(()=>[l("2. 多头注意力")]),_:1})]),e("li",null,[n(i,{to:"#_3-point-wise-ffn"},{default:t(()=>[l("3. Point-wise FFN")]),_:1})]),e("li",null,[n(i,{to:"#_4-模型参数计算"},{default:t(()=>[l("4. 模型参数计算")]),_:1})]),e("li",null,[n(i,{to:"#_5-transformer结构"},{default:t(()=>[l("5 transformer结构")]),_:1})]),e("li",null,[n(i,{to:"#_6-和rnn对比"},{default:t(()=>[l("6. 和rnn对比")]),_:1})])])]),y,e("ul",null,[e("li",null,[e("p",null,[e("a",B,[l("详细的解释"),n(a)])])]),e("li",null,[e("p",null,[e("a",N,[l("d2l网址"),n(a)])])])]),I])}const C=o(v,[["render",z],["__file","模型 -- Transformer.html.vue"]]),E=JSON.parse('{"path":"/AI/NLP/%E6%A8%A1%E5%9E%8B%20--%20Transformer.html","title":"基础 -- Transformer","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.资料网址：","slug":"_0-资料网址","link":"#_0-资料网址","children":[]},{"level":2,"title":"0. positional encodering","slug":"_0-positional-encodering","link":"#_0-positional-encodering","children":[{"level":3,"title":"0.1 为什么需要positional encodering，普通的为何不行","slug":"_0-1-为什么需要positional-encodering-普通的为何不行","link":"#_0-1-为什么需要positional-encodering-普通的为何不行","children":[]},{"level":3,"title":"0.2 位置编码需要的特性","slug":"_0-2-位置编码需要的特性","link":"#_0-2-位置编码需要的特性","children":[]},{"level":3,"title":"0.3 当前的位置编码","slug":"_0-3-当前的位置编码","link":"#_0-3-当前的位置编码","children":[]}]},{"level":2,"title":"1. self-attention","slug":"_1-self-attention","link":"#_1-self-attention","children":[{"level":3,"title":"1.1 归一化","slug":"_1-1-归一化","link":"#_1-1-归一化","children":[]},{"level":3,"title":"1.2 如何mask","slug":"_1-2-如何mask","link":"#_1-2-如何mask","children":[]}]},{"level":2,"title":"2. 多头注意力","slug":"_2-多头注意力","link":"#_2-多头注意力","children":[]},{"level":2,"title":"3. Point-wise FFN","slug":"_3-point-wise-ffn","link":"#_3-point-wise-ffn","children":[]},{"level":2,"title":"4. 模型参数计算","slug":"_4-模型参数计算","link":"#_4-模型参数计算","children":[]},{"level":2,"title":"5 transformer结构","slug":"_5-transformer结构","link":"#_5-transformer结构","children":[]},{"level":2,"title":"6. 和rnn对比","slug":"_6-和rnn对比","link":"#_6-和rnn对比","children":[]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/NLP/模型 -- Transformer.md"}');export{C as comp,E as data};
