import{_ as r,r as i,o as c,c as _,b as e,e as a,w as t,d as l,a as o}from"./app-DFklLwn2.js";const h="/online_notesV2/assets/image-20220628205443548-Dag-7rXR.png",m="/online_notesV2/assets/image-20220628205603236-DvN1IjWu.png",d="/online_notesV2/assets/image-20220628112432023-16563866733421-qjqkp6aX.png",p="/online_notesV2/assets/image-20220628113609658-16563873709792-D01g0mfP.png",g="/online_notesV2/assets/image-20220628113933578-16563875756703-CuR_yQ_9.png",u="/online_notesV2/assets/image-20220628114419631-16563878611514-CdGCSTZv.png",f="/online_notesV2/assets/image-20220628205641253-CUVowxFS.png",b="/online_notesV2/assets/image-20220628205754749-DSsHSksq.png",y="/online_notesV2/assets/image-20220628205943844-WuJU1_pD.png",N="/online_notesV2/assets/image-20220628210135431-DRCca0Q5.png",k={},B=e("h1",{id:"基础神经网络-layer-norm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#基础神经网络-layer-norm"},[e("span",null,"基础神经网络--Layer Norm")])],-1),x={class:"table-of-contents"},v=e("h2",{id:"_0-资料网址",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_0-资料网址"},[e("span",null,"0.资料网址：")])],-1),z={href:"https://zhuanlan.zhihu.com/p/33173246",target:"_blank",rel:"noopener noreferrer"},V=o('<h2 id="归一化的通用框架" tabindex="-1"><a class="header-anchor" href="#归一化的通用框架"><span>归一化的通用框架</span></a></h2><img src="'+h+'" alt="image-20220628205443548" style="zoom:67%;"><h2 id="_1-bn-一个batch进行scale" tabindex="-1"><a class="header-anchor" href="#_1-bn-一个batch进行scale"><span>1. BN -- 一个batch进行scale</span></a></h2><img src="'+m+'" alt="image-20220628205603236" style="zoom:50%;">',4),L={href:"http://proceedings.mlr.press/v37/ioffe15.pdf",target:"_blank",rel:"noopener noreferrer"},C={href:"https://blog.csdn.net/happynear/article/details/44238541",target:"_blank",rel:"noopener noreferrer"},S={href:"https://proceedings.neurips.cc/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf",target:"_blank",rel:"noopener noreferrer"},w=o('<h3 id="_1-1-为什么需要bn" tabindex="-1"><a class="header-anchor" href="#_1-1-为什么需要bn"><span>1.1 为什么需要BN</span></a></h3><ul><li><p>BN是对于每个激活函数的操作(可能在激活函数之前，也可能在激活函数之后)</p></li><li><p>需要BN的原因是所谓的Internal Covariate Shift</p><ul><li><img src="'+d+'" alt="image-20220628112432023" style="zoom:50%;"></li><li>ICS导致了如下的问题： <ul><li>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</li><li>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。 <ul><li><img src="'+p+'" alt="image-20220628113609658" style="zoom:50%;"></li></ul></li><li>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li></ul></li></ul></li></ul><h3 id="_1-2-bn的算法" tabindex="-1"><a class="header-anchor" href="#_1-2-bn的算法"><span>1.2 BN的算法</span></a></h3><img src="'+g+'" alt="image-20220628113933578" style="zoom:67%;"><ul><li>即对每个激活函数的输入/输出x，将x先Z归一化成y，而后把y替换掉x。</li><li>而这个归一化，是使用一个batch内x的均值和方差来进行的，因此叫batch norm</li><li>而为了给网络更大的能力，设置了两个可学习的函数，把Z归一化后的y再scale和bias，成为最终的替代</li></ul><h3 id="_1-3-评估的时候" tabindex="-1"><a class="header-anchor" href="#_1-3-评估的时候"><span>1.3 评估的时候</span></a></h3><img src="'+u+'" alt="image-20220628114419631" style="zoom:67%;">',7),D=e("h2",{id:"_2-layer-norm-一层scale",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-layer-norm-一层scale"},[e("span",null,"2. Layer Norm -- 一层scale")])],-1),E=e("img",{src:f,alt:"image-20220628205641253",style:{zoom:"67%"}},null,-1),I={href:"https://arxiv.org/pdf/1607.06450.pdf",target:"_blank",rel:"noopener noreferrer"},R=o('<h2 id="_3-weight-norm-使用权重对数据scale" tabindex="-1"><a class="header-anchor" href="#_3-weight-norm-使用权重对数据scale"><span>3. Weight Norm -- 使用权重对数据scale</span></a></h2><img src="'+b+'" alt="image-20220628205754749" style="zoom:67%;"><img src="'+y+'" alt="image-20220628205943844" style="zoom:67%;"><img src="'+N+'" alt="image-20220628210135431" style="zoom:67%;">',4);function W(A,T){const n=i("router-link"),s=i("ExternalLinkIcon");return c(),_("div",null,[B,e("nav",x,[e("ul",null,[e("li",null,[a(n,{to:"#_0-资料网址"},{default:t(()=>[l("0.资料网址：")]),_:1})]),e("li",null,[a(n,{to:"#归一化的通用框架"},{default:t(()=>[l("归一化的通用框架")]),_:1})]),e("li",null,[a(n,{to:"#_1-bn-一个batch进行scale"},{default:t(()=>[l("1. BN -- 一个batch进行scale")]),_:1}),e("ul",null,[e("li",null,[a(n,{to:"#_1-1-为什么需要bn"},{default:t(()=>[l("1.1 为什么需要BN")]),_:1})]),e("li",null,[a(n,{to:"#_1-2-bn的算法"},{default:t(()=>[l("1.2 BN的算法")]),_:1})]),e("li",null,[a(n,{to:"#_1-3-评估的时候"},{default:t(()=>[l("1.3 评估的时候")]),_:1})])])]),e("li",null,[a(n,{to:"#_2-layer-norm-一层scale"},{default:t(()=>[l("2. Layer Norm -- 一层scale")]),_:1})]),e("li",null,[a(n,{to:"#_3-weight-norm-使用权重对数据scale"},{default:t(()=>[l("3. Weight Norm -- 使用权重对数据scale")]),_:1})])])]),v,e("ul",null,[e("li",null,[e("a",z,[l("知乎关于norm的好专题文章"),a(s)])])]),V,e("ul",null,[e("li",null,[e("a",L,[l("论文Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"),a(s)])]),e("li",null,[e("a",C,[l("好的阅读笔记"),a(s)])]),e("li",null,[e("a",S,[l("论文Understanding Batch Normalization"),a(s)])])]),w,l(" - 在训练的最后一个epoch，需要记录下所有mini-batch的每个的mean和Var - 最后统计得出这个激活函数对整个epoch的mean和Var。在评估过程中，使用整体mean和Var进行归一化 "),D,E,e("ul",null,[e("li",null,[e("a",I,[l("论文Layer Normalization"),a(s)])])]),R])}const q=r(k,[["render",W],["__file","神经网络--Normalization Layers.html.vue"]]),P=JSON.parse('{"path":"/AI/Basic/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html","title":"基础神经网络--Layer Norm","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.资料网址：","slug":"_0-资料网址","link":"#_0-资料网址","children":[]},{"level":2,"title":"归一化的通用框架","slug":"归一化的通用框架","link":"#归一化的通用框架","children":[]},{"level":2,"title":"1. BN -- 一个batch进行scale","slug":"_1-bn-一个batch进行scale","link":"#_1-bn-一个batch进行scale","children":[{"level":3,"title":"1.1 为什么需要BN","slug":"_1-1-为什么需要bn","link":"#_1-1-为什么需要bn","children":[]},{"level":3,"title":"1.2 BN的算法","slug":"_1-2-bn的算法","link":"#_1-2-bn的算法","children":[]},{"level":3,"title":"1.3 评估的时候","slug":"_1-3-评估的时候","link":"#_1-3-评估的时候","children":[]}]},{"level":2,"title":"2. Layer Norm -- 一层scale","slug":"_2-layer-norm-一层scale","link":"#_2-layer-norm-一层scale","children":[]},{"level":2,"title":"3. Weight Norm -- 使用权重对数据scale","slug":"_3-weight-norm-使用权重对数据scale","link":"#_3-weight-norm-使用权重对数据scale","children":[]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/Basic/神经网络--Normalization Layers.md"}');export{q as comp,P as data};
