import{_ as o,r,o as _,c as u,b as l,e,w as a,d as n,a as i}from"./app-DFklLwn2.js";const d="/online_notesV2/assets/architecture-rnn-ltr-DW5rEqrH.png",c="/online_notesV2/assets/image-20220626143653270-DHsgR_xl.png",p="/online_notesV2/assets/image-20220626162631825-By2qavl8.png",h="/online_notesV2/assets/1yBXV9o5q7L_CvY7quJt3WQ-BJ8WqO-3.png",g="/online_notesV2/assets/image-20210511204551220-CazwQ_Pj.png",m="/online_notesV2/assets/image-20220626172906049-Byyi0RVk.png",f="/online_notesV2/assets/gru-CUVXo6Cr.png",b="/online_notesV2/assets/image-20220626172842386-Dq9pcZ26.png",N="/online_notesV2/assets/image-20220626174043183-B8gPbBt8.png",x="/online_notesV2/assets/image-20220626145349858-DWuCSya3.png",R="/online_notesV2/assets/image-20220626174913378-BytcAE7M.png",k={},w=l("h1",{id:"神经网络-rnn-lstm-gru",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#神经网络-rnn-lstm-gru"},[l("span",null,"神经网络--RNN|LSTM|GRU")])],-1),v={class:"table-of-contents"},y=l("h2",{id:"_0-资料网址",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#_0-资料网址"},[l("span",null,"0.资料网址：")])],-1),L={href:"https://www.jiqizhixin.com/articles/2018-12-18-12",target:"_blank",rel:"noopener noreferrer"},S={href:"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21",target:"_blank",rel:"noopener noreferrer"},M={href:"https://paddlepedia.readthedocs.io/en/latest/index.html",target:"_blank",rel:"noopener noreferrer"},T={href:"https://www.deeplearningbook.org/",target:"_blank",rel:"noopener noreferrer"},U={href:"https://www.youtube.com/c/3blue1brown",target:"_blank",rel:"noopener noreferrer"},G={href:"https://www.youtube.com/watch?v=CpD9XlTu3ys",target:"_blank",rel:"noopener noreferrer"},V={href:"https://easyai.tech/ai-definition/lstm/",target:"_blank",rel:"noopener noreferrer"},B=i('<h2 id="_1-rnn" tabindex="-1"><a class="header-anchor" href="#_1-rnn"><span>1. RNN</span></a></h2><h3 id="_1-1-rnn的动机" tabindex="-1"><a class="header-anchor" href="#_1-1-rnn的动机"><span>1.1 RNN的动机</span></a></h3><ul><li>需要处理变长的序列</li><li>要学习到输入的时间的依赖关系</li></ul><h3 id="_1-2-rnn的结构和公式" tabindex="-1"><a class="header-anchor" href="#_1-2-rnn的结构和公式"><span>1.2 RNN的结构和公式</span></a></h3><h4 id="结构" tabindex="-1"><a class="header-anchor" href="#结构"><span>结构</span></a></h4><p><img src="'+d+'" alt="architecture-rnn-ltr"></p><ul><li>上述结构图参考(https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#architecture)</li></ul><h4 id="公式" tabindex="-1"><a class="header-anchor" href="#公式"><span>公式</span></a></h4><img src="'+c+'" alt="image-20220626143653270" style="zoom:25%;"><h4 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h4><ul><li><strong>RNN由输入、隐藏层、输出组成</strong></li><li><strong>输入是逐个输入</strong></li><li><strong>隐藏层--与当前输入、上一个隐藏层 有关</strong>，使用的激活函数为tanh <ul><li>__使用tanh__是因为在级联的时候，如果有一个输入值特别大，而没有tanh归一化[-1,1]的效果的话，连乘下去这个值到最后将会非常大，那么别的比较小的数就没有任何意义了</li></ul></li><li><strong>输出--只与当前隐藏层有关</strong></li></ul><h3 id="_1-3-rnn的优点、缺点" tabindex="-1"><a class="header-anchor" href="#_1-3-rnn的优点、缺点"><span>1.3 RNN的优点、缺点</span></a></h3><table><thead><tr><th>优点:</th><th>缺点：</th></tr></thead><tbody><tr><td>模型大小不随输入长度的增加</td><td>计算缓慢</td></tr><tr><td>可以处理任意长序列</td><td>存在梯度消失和梯度爆炸的问题</td></tr><tr><td>考虑了时间的依赖</td><td></td></tr><tr><td>参数随时间共享</td><td></td></tr></tbody></table><h3 id="_1-4-rnn中的梯度消失和爆炸" tabindex="-1"><a class="header-anchor" href="#_1-4-rnn中的梯度消失和爆炸"><span>1.4 <mark>RNN中的梯度消失和爆炸</mark></span></a></h3><img src="'+p+'" alt="image-20220626162631825" style="zoom:33%;"><ul><li><strong>梯度爆炸：学习到的参数矩阵的最大奇异值大于1</strong><ul><li>可以使用梯度裁剪解决(clipping)</li></ul></li><li><strong>梯度消失：学习到的参数矩阵的最大奇异值小于1</strong><ul><li>当最大奇异值小于1的时候，往前传播的时候，随着t越来越大，梯度越来越小，越往前神经网络越不更新，削弱了RNN捕获长距离的能力</li></ul></li></ul><h2 id="_2-lstm" tabindex="-1"><a class="header-anchor" href="#_2-lstm"><span>2.LSTM</span></a></h2><h3 id="_2-1-lstm的动机" tabindex="-1"><a class="header-anchor" href="#_2-1-lstm的动机"><span>2.1 LSTM的动机</span></a></h3><p>__有选择性的输入__以缓解梯度消失的问题</p><h3 id="_2-2-lstm的结构和公式" tabindex="-1"><a class="header-anchor" href="#_2-2-lstm的结构和公式"><span>2.2 LSTM的结构和公式</span></a></h3><h4 id="结构-1" tabindex="-1"><a class="header-anchor" href="#结构-1"><span>结构</span></a></h4><img src="'+h+'" alt="img" style="zoom:50%;"><p><img src="'+g+'" alt="image-20210511204551220"></p><h4 id="公式-注意-公式中激活函数内的相加不是相加-而是向量拼接" tabindex="-1"><a class="header-anchor" href="#公式-注意-公式中激活函数内的相加不是相加-而是向量拼接"><span>公式--注意，公式中激活函数内的相加不是相加，而是向量拼接</span></a></h4><img src="'+m+'" alt="image-20220626172906049" style="zoom:80%;"><h4 id="总结-3门3态" tabindex="-1"><a class="header-anchor" href="#总结-3门3态"><span>总结--3门3态</span></a></h4><ul><li><p><strong>门</strong>：</p><ul><li>都与 当前输入 和 上一时刻隐藏层 相关</li><li>都使用sigmoid作为激活函数</li><li>都有可学习参数</li></ul></li><li><p>输入门</p><ul><li>用于 细胞态 中调控 暂态 的输入</li></ul></li><li><p>遗忘门</p><ul><li>用于 细胞态 中调控 上一时刻 细胞态 的输入</li></ul></li><li><p>输出门</p><ul><li>用于从 细胞态 生成 当前时刻的 隐藏层</li></ul></li><li><p><strong>态</strong>：</p></li><li><p>暂态</p><ul><li>与 当前输入 和 上一时刻隐藏层 相关</li><li>使用tanh作为激活函数</li><li>有可学习参数</li></ul></li><li><p>细胞态</p><ul><li>由 暂态 与 上一时刻细胞态 决定</li><li>由 输入门 和 遗忘门 调控输入</li><li>无激活函数</li><li>无可学习参数</li></ul></li><li><p>隐藏层</p><ul><li>直接由 细胞态 经过 tanh 后经由 输出门 调控生成</li><li>使用tanh作为激活函数</li><li>无可学习参数</li></ul></li></ul><h2 id="_3-gru" tabindex="-1"><a class="header-anchor" href="#_3-gru"><span>3.GRU</span></a></h2><h3 id="_3-1-gru的动机" tabindex="-1"><a class="header-anchor" href="#_3-1-gru的动机"><span>3.1 GRU的动机</span></a></h3><h3 id="_3-2-gru的结构和公式" tabindex="-1"><a class="header-anchor" href="#_3-2-gru的结构和公式"><span>3.2 GRU的结构和公式</span></a></h3><h4 id="结构-2" tabindex="-1"><a class="header-anchor" href="#结构-2"><span>结构</span></a></h4><img src="'+h+'" alt="img" style="zoom:50%;"><img src="'+f+'" alt="gru" style="zoom:50%;"><h4 id="公式-注意-公式中激活函数内的相加不是相加-而是向量拼接-1" tabindex="-1"><a class="header-anchor" href="#公式-注意-公式中激活函数内的相加不是相加-而是向量拼接-1"><span>公式--注意，公式中激活函数内的相加不是相加，而是向量拼接</span></a></h4><img src="'+b+'" alt="image-20220626172842386" style="zoom:80%;"><h4 id="总结-2门2态" tabindex="-1"><a class="header-anchor" href="#总结-2门2态"><span>总结--2门2态</span></a></h4>',36),z=l("ul",null,[l("li",null,[l("p",null,[l("strong",null,"门"),n("：")]),l("ul",null,[l("li",null,"都与当前输入和上一时刻的隐藏层有关，对于 当前输入 和 上一时刻隐藏层 都有可学习参数"),l("li",null,"都使用sigmoid激活函数进行逐个元素相乘，相当于门控")])]),l("li",null,[l("p",null,"重置门"),l("ul",null,[l("li",null,"负责在 暂态 中遗忘 上一时刻的隐藏层")])]),l("li",null,[l("p",null,"更新门"),l("ul",null,[l("li",null,"负责在 隐藏层 中调控 暂态 和 上一时刻隐藏层 的比例")])]),l("li",null,[l("p",null,[l("strong",null,"态"),n("：")]),l("ul",null,[l("li",null,"GRU只有暂态和隐藏层")])]),l("li",null,[l("p",null,"暂态"),l("ul",null,[l("li",null,[n("和RNN的隐藏层类似，只是 参数"),l("span",{class:"katex"},[l("span",{class:"katex-mathml"},[l("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[l("semantics",null,[l("mrow",null,[l("mo",null,"∗")]),l("annotation",{encoding:"application/x-tex"},"*")])])]),l("span",{class:"katex-html","aria-hidden":"true"},[l("span",{class:"base"},[l("span",{class:"strut",style:{height:"0.4653em"}}),l("span",{class:"mord"},"∗")])])]),n("​​​上一时刻隐藏层 需要经 重置门 调控")]),l("li",null,"tanh为激活函数"),l("li",null,"有可学习参数")])]),l("li",null,[l("p",null,"隐藏层"),l("ul",null,[l("li",null,"由 暂态 和 上一时刻隐藏层 经由 更新门 调控输入比例"),l("li",null,"无激活函数"),l("li",null,"无可学习参数")])])],-1),E=i('<h2 id="_4-其他" tabindex="-1"><a class="header-anchor" href="#_4-其他"><span>4. 其他</span></a></h2><h3 id="_4-0-比较" tabindex="-1"><a class="header-anchor" href="#_4-0-比较"><span>4.0 比较</span></a></h3><h4 id="lstm与gru" tabindex="-1"><a class="header-anchor" href="#lstm与gru"><span>LSTM与GRU</span></a></h4><ul><li><p>LSTM能够解决循环神经网络因长期依赖带来的梯度消失和梯度爆炸问题，但是LSTM有三个不同的门，参数较多，训练起来比较困难。</p></li><li><p>GRU只含有两个门控结构，且在超参数全部调优的情况下，二者性能相当，但是GRU结构更为简单，训练样本较少，易实现。</p></li><li><img src="'+N+'" alt="image-20220626174043183"></li></ul><h3 id="_4-1-常使用激活函数" tabindex="-1"><a class="header-anchor" href="#_4-1-常使用激活函数"><span>4.1 常使用激活函数</span></a></h3><img src="'+x+'" alt="image-20220626145349858" style="zoom:80%;"><h4 id="为何非门控单元的可学习参数-选择tanh而不也是sigmoid" tabindex="-1"><a class="header-anchor" href="#为何非门控单元的可学习参数-选择tanh而不也是sigmoid"><span>为何非门控单元的可学习参数，选择tanh而不也是sigmoid</span></a></h4><ul><li>在输入为0附近，tanh的梯度比sigmoid更大，学习收敛更快</li></ul><h4 id="可否使用relu作为激活函数" tabindex="-1"><a class="header-anchor" href="#可否使用relu作为激活函数"><span>可否使用RELU作为激活函数</span></a></h4><ul><li>使用RELU作为激活函数，则失去了tanh的约束，会引发梯度的消失和爆炸</li><li>如果可学习参数初始化在单位阵附近，则可能可以使用</li></ul><h3 id="_4-2-参数量的计算" tabindex="-1"><a class="header-anchor" href="#_4-2-参数量的计算"><span>4.2 参数量的计算</span></a></h3><p>参考pytorch中的计算：https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM</p><h3 id="_4-3-代码的书写" tabindex="-1"><a class="header-anchor" href="#_4-3-代码的书写"><span>4.3 代码的书写</span></a></h3>',13),C={href:"https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM",target:"_blank",rel:"noopener noreferrer"},q=i('<h3 id="_4-4-不同的整合方式" tabindex="-1"><a class="header-anchor" href="#_4-4-不同的整合方式"><span>4.4 不同的整合方式：</span></a></h3><p>https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview</p><img src="'+R+'" alt="image-20220626174913378" style="zoom:50%;"><h2 id="_5-长期依赖以及梯度消失解决" tabindex="-1"><a class="header-anchor" href="#_5-长期依赖以及梯度消失解决"><span>5. 长期依赖以及梯度消失解决</span></a></h2><p>https://www.cnblogs.com/bonelee/p/10475453.html</p><p>https://zhuanlan.zhihu.com/p/47780305</p><p>https://www.zhihu.com/question/317594964</p><p>LSTM为什么能解决梯度消失：https://zhuanlan.zhihu.com/p/451018380</p>',8);function D(A,I){const t=r("router-link"),s=r("ExternalLinkIcon");return _(),u("div",null,[w,l("nav",v,[l("ul",null,[l("li",null,[e(t,{to:"#_0-资料网址"},{default:a(()=>[n("0.资料网址：")]),_:1})]),l("li",null,[e(t,{to:"#_1-rnn"},{default:a(()=>[n("1. RNN")]),_:1}),l("ul",null,[l("li",null,[e(t,{to:"#_1-1-rnn的动机"},{default:a(()=>[n("1.1 RNN的动机")]),_:1})]),l("li",null,[e(t,{to:"#_1-2-rnn的结构和公式"},{default:a(()=>[n("1.2 RNN的结构和公式")]),_:1})]),l("li",null,[e(t,{to:"#_1-3-rnn的优点、缺点"},{default:a(()=>[n("1.3 RNN的优点、缺点")]),_:1})]),l("li",null,[e(t,{to:"#_1-4-rnn中的梯度消失和爆炸"},{default:a(()=>[n("1.4 RNN中的梯度消失和爆炸")]),_:1})])])]),l("li",null,[e(t,{to:"#_2-lstm"},{default:a(()=>[n("2.LSTM")]),_:1}),l("ul",null,[l("li",null,[e(t,{to:"#_2-1-lstm的动机"},{default:a(()=>[n("2.1 LSTM的动机")]),_:1})]),l("li",null,[e(t,{to:"#_2-2-lstm的结构和公式"},{default:a(()=>[n("2.2 LSTM的结构和公式")]),_:1})])])]),l("li",null,[e(t,{to:"#_3-gru"},{default:a(()=>[n("3.GRU")]),_:1}),l("ul",null,[l("li",null,[e(t,{to:"#_3-1-gru的动机"},{default:a(()=>[n("3.1 GRU的动机")]),_:1})]),l("li",null,[e(t,{to:"#_3-2-gru的结构和公式"},{default:a(()=>[n("3.2 GRU的结构和公式")]),_:1})])])]),l("li",null,[e(t,{to:"#_4-其他"},{default:a(()=>[n("4. 其他")]),_:1}),l("ul",null,[l("li",null,[e(t,{to:"#_4-0-比较"},{default:a(()=>[n("4.0 比较")]),_:1})]),l("li",null,[e(t,{to:"#_4-1-常使用激活函数"},{default:a(()=>[n("4.1 常使用激活函数")]),_:1})]),l("li",null,[e(t,{to:"#_4-2-参数量的计算"},{default:a(()=>[n("4.2 参数量的计算")]),_:1})]),l("li",null,[e(t,{to:"#_4-3-代码的书写"},{default:a(()=>[n("4.3 代码的书写")]),_:1})]),l("li",null,[e(t,{to:"#_4-4-不同的整合方式"},{default:a(()=>[n("4.4 不同的整合方式：")]),_:1})])])]),l("li",null,[e(t,{to:"#_5-长期依赖以及梯度消失解决"},{default:a(()=>[n("5. 长期依赖以及梯度消失解决")]),_:1})])])]),y,l("ul",null,[l("li",null,[l("a",L,[n("核心机器之心的post"),e(s)])]),l("li",null,[l("a",S,[n("三个神经网络的动图解释"),e(s)])]),l("li",null,[l("a",M,[n("飞桨文档"),e(s)])]),l("li",null,[l("a",T,[n("Deep Learning Book"),e(s)])]),l("li",null,[l("a",U,[n("参考视频--可视化数学非常好的教学"),e(s)])]),l("li",null,[l("a",G,[n("SVD分解的图解视频"),e(s)])]),l("li",null,[l("a",V,[n("介绍LSTM"),e(s)])])]),B,z,E,l("ul",null,[l("li",null,[l("a",C,[n("官方代码"),e(s)])])]),q])}const J=o(k,[["render",D],["__file","神经网络--RNN_LSTM_GRU.html.vue"]]),P=JSON.parse('{"path":"/AI/Basic/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN_LSTM_GRU.html","title":"神经网络--RNN|LSTM|GRU","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"0.资料网址：","slug":"_0-资料网址","link":"#_0-资料网址","children":[]},{"level":2,"title":"1. RNN","slug":"_1-rnn","link":"#_1-rnn","children":[{"level":3,"title":"1.1 RNN的动机","slug":"_1-1-rnn的动机","link":"#_1-1-rnn的动机","children":[]},{"level":3,"title":"1.2 RNN的结构和公式","slug":"_1-2-rnn的结构和公式","link":"#_1-2-rnn的结构和公式","children":[]},{"level":3,"title":"1.3 RNN的优点、缺点","slug":"_1-3-rnn的优点、缺点","link":"#_1-3-rnn的优点、缺点","children":[]},{"level":3,"title":"1.4 RNN中的梯度消失和爆炸","slug":"_1-4-rnn中的梯度消失和爆炸","link":"#_1-4-rnn中的梯度消失和爆炸","children":[]}]},{"level":2,"title":"2.LSTM","slug":"_2-lstm","link":"#_2-lstm","children":[{"level":3,"title":"2.1 LSTM的动机","slug":"_2-1-lstm的动机","link":"#_2-1-lstm的动机","children":[]},{"level":3,"title":"2.2 LSTM的结构和公式","slug":"_2-2-lstm的结构和公式","link":"#_2-2-lstm的结构和公式","children":[]}]},{"level":2,"title":"3.GRU","slug":"_3-gru","link":"#_3-gru","children":[{"level":3,"title":"3.1 GRU的动机","slug":"_3-1-gru的动机","link":"#_3-1-gru的动机","children":[]},{"level":3,"title":"3.2 GRU的结构和公式","slug":"_3-2-gru的结构和公式","link":"#_3-2-gru的结构和公式","children":[]}]},{"level":2,"title":"4. 其他","slug":"_4-其他","link":"#_4-其他","children":[{"level":3,"title":"4.0 比较","slug":"_4-0-比较","link":"#_4-0-比较","children":[]},{"level":3,"title":"4.1 常使用激活函数","slug":"_4-1-常使用激活函数","link":"#_4-1-常使用激活函数","children":[]},{"level":3,"title":"4.2 参数量的计算","slug":"_4-2-参数量的计算","link":"#_4-2-参数量的计算","children":[]},{"level":3,"title":"4.3 代码的书写","slug":"_4-3-代码的书写","link":"#_4-3-代码的书写","children":[]},{"level":3,"title":"4.4 不同的整合方式：","slug":"_4-4-不同的整合方式","link":"#_4-4-不同的整合方式","children":[]}]},{"level":2,"title":"5. 长期依赖以及梯度消失解决","slug":"_5-长期依赖以及梯度消失解决","link":"#_5-长期依赖以及梯度消失解决","children":[]}],"git":{"updatedTime":1706457681000,"contributors":[{"name":"henryhuanghenry","email":"henryhuanghenry@outlook.com","commits":1}]},"filePathRelative":"AI/Basic/神经网络--RNN+LSTM+GRU.md"}');export{J as comp,P as data};
