# 基础 -- 词模型

[[TOC]]

## 0.资料网址：

- [NLP 领域里的8 种文本表示方式及优缺点](https://easyai.tech/blog/nlp-%E9%A2%86%E5%9F%9F%E9%87%8C%E7%9A%848-%E7%A7%8D%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E6%96%B9%E5%BC%8F%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9/)
- [word2vec的数学原理](https://blog.csdn.net/itplus/article/details/37969519)
- [glove详解](http://www.fanyeong.com/2018/02/19/glove-in-detail/#comment-1462)

## 1. 针对文档的编码

### 1.1 BOW 词袋模型

所有文档的词的集合形成空间，每个文档的向量就是在这个空间中，统计每个词语在该文档中出现的频率

- 缺点
  - 无法区分常用词和关键词

### 1.2 TF-IDF

有文档，以及文档的集合；可将文档表征为关于词的向量。

向量的每个值代表了一个词，数值用TF-IDF计算。

- TF代表该词语在该文档的频率
- IDF为逆文档频率，$log(\frac{totalDoc}{appearDoc+1})$​

- 优点
  - 实现简单
- 缺点
  - 不能反映词的位置信息
  - 与语料库相关



## 2.针对单词本身的编码

### 2.1 共现矩阵+降维

- 使用窗口构成n-gram
- 在同一窗口内的词，两两称为共现
- 用一个矩阵，记录共现的次数
- 然后对该大型矩阵奇异值分解，进行降维，得到词语的矩阵

- 优点
  - 考虑了出现的顺序
- 缺点
  - 当词语较多时，构建共现矩阵与降维较为麻烦

### 2.2 word2vec

![img](./pic/1cuOmGT7NevP9oJFJfVpRKA.png)

#### skip-gram

对于每个词，目标词的梯度要反向传播K次的。即一个词要训练K次。

- 对低频词有好处。

#### cbow

对于每个词，只训练一次，为这个词的梯度反向传播到K个词。

- 因为预测是周围的词加权在一起，所以低频次起的作用可能被高频词覆盖了。

#### 层次化softmax

https://zhuanlan.zhihu.com/p/56139075

http://www.hankcs.com/nlp/word2vec.html#respond

- 根据词频生成一颗哈夫曼树
- 而后哈夫曼树向左向右走是由参数来决定的。
  - 每次向左还是向右，根据这一层的概率来决定。
- 层次softmax在一次前向+反向传播计算过程中，只需要计算从根节点到ground truth  label对应的word这条路径上的概率，根据极大似然原理，使这条路径概率最大化，而不用每一个叶子节点都去计算一遍从根节点到叶节点路径的概率。和负采样一样，负采样是挑选了N个单词作为负例，然后根据极大似然估计原理，使得ground truth label对应的word成为正例的概率最大，并使得这N个单词成为负例的概率最大。而层次softmax仅仅是使得ground  truth label这条路径的概率最大，就有点像lstm+crf模型中用到的负对数似然损失一样，每一次前向+反向的过程就是使得ground  truth对应的标签序列这条路径概率最大

#### 负采样

https://www.cnblogs.com/pinard/p/7249903.html

### 2.3 Glove

基于词向量的点积去拟合共现概率比https://blog.51cto.com/u_15054042/2564022

### 2.4 ELMo

![img](./pic/elmo-2.png)

https://zhuanlan.zhihu.com/p/63115885

利用两个LSTM，一个编码上文信息，一个编码下文信息；每个目标单词利用上下文信息去预测。

词向量为：多层+上+下文信息的两个hidden state，加上单词本身的Hidden state

- 缺点，没有transformer的信息抽取能力强。
- 双向拼接融合特征的方法可能没有那么的好

### 2.5 GPT

### 2.6 Bert

- transformer的encoder
- WordPiece embeddings
- Masked LM (15%)
  - fine-tune的时候mask不会出现：80%mask，10换其他，10不变
- Next Sentence Prediction (NSP)

### 2.7 roberta

- Byte-Pair Encoding (BPE)
- Dynamic Masking
- Training with large batches
- 更长的句子using individual sentences hurts performance on downstream tasks
-  removing the NSP loss matches or slightly improves downstream task performance

### 2.8 albert

减少参数数量，加速训练

- Factorized embedding parameterization
  -  Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space
- Cross-layer parameter sharing
- Inter-sentence coherence loss 预测是否为下一个句子
